                if self.features is not None and not isinstance(self.vectorizer,
                                                        DictVectorizer):
            raise ValueError('FeatureSets can only be iterated through if they'
                             ' use a DictVectorizer for their feature '
                             'vectorizer.')
        for id_, label_, feats in zip(self.ids, self.labels, self.features):
                        if ids is not None and (id_ in ids) == inverse:
                continue
                        if labels is not None and (label_ in labels) == inverse:
                continue
            feat_dict = self.vectorizer.inverse_transform(feats)[0]
            if features is not None:
                feat_dict = {name: value for name, value in
                             iteritems(feat_dict) if
                             (inverse != (name in features or
                                          name.split('=', 1)[0] in features))}
            elif not inverse:
                feat_dict = {}
            yield id_, label_, feat_dict
    def __sub__(self, other):
                new_set = deepcopy(self)
        new_set.filter(features=other.vectorizer.feature_names_,
                       inverse=True)
        return new_set
    @property
    def has_labels(self):
                if self.labels is not None:
            return not (np.issubdtype(self.labels.dtype, float) and
                        np.isnan(np.min(self.labels)))
        else:
            return False
    def __str__(self):
                return str(self.__dict__)
    def __repr__(self):
                return repr(self.__dict__)
    def __getitem__(self, value):
                        if isinstance(value, slice):
            sliced_ids = self.ids[value]
            sliced_feats = (self.features[value] if self.features is not None
                            else None)
            sliced_labels = (self.labels[value] if self.labels is not None
                             else None)
            return FeatureSet('{}_{}'.format(self.name, value), sliced_ids,
                              features=sliced_feats, labels=sliced_labels,
                              vectorizer=self.vectorizer)
        else:
            label = self.labels[value] if self.labels is not None else None
            feats = self.features[value, :]
            features = (self.vectorizer.inverse_transform(feats)[0] if
                        self.features is not None else {})
            return self.ids[value], label, features
    @staticmethod
    def split_by_ids(fs, ids_for_split1, ids_for_split2=None):
        
                                                        ids1 = fs.ids[ids_for_split1]
        labels1 = fs.labels[ids_for_split1]
        features1 = fs.features[ids_for_split1]
        if ids_for_split2 is None:
            ids2 = fs.ids[~np.in1d(fs.ids, ids_for_split1)]
            labels2 = fs.labels[~np.in1d(fs.ids, ids_for_split1)]
            features2 = fs.features[~np.in1d(fs.ids, ids_for_split1)]
        else:
            ids2 = fs.ids[ids_for_split2]
            labels2 = fs.labels[ids_for_split2]
            features2 = fs.features[ids_for_split2]
        fs1 = FeatureSet('{}_1'.format(fs.name),
                         ids1,
                         labels=labels1,
                         features=features1,
                         vectorizer=fs.vectorizer)
        fs2 = FeatureSet('{}_2'.format(fs.name),
                         ids2,
                         labels=labels2,
                         features=features2,
                         vectorizer=fs.vectorizer)
        return fs1, fs2
    @staticmethod
    def from_data_frame(df, name, labels_column=None, vectorizer=None):
                if labels_column:
            feature_columns = [column for column in df.columns if column != labels_column]
            labels = df[labels_column].tolist()
        else:
            feature_columns = df.columns
            labels = None
        features = df[feature_columns].to_dict(orient='records')
        return FeatureSet(name,
                          ids=df.index.tolist(),
                          labels=labels,
                          features=features,
                          vectorizer=vectorizer)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import csv
import json
import logging
import re
import sys
from csv import DictReader
from itertools import chain, islice
from io import open, BytesIO, StringIO
import numpy as np
from bs4 import UnicodeDammit
from six import iteritems, PY2, PY3, string_types, text_type
from six.moves import map, zip
from sklearn.feature_extraction import FeatureHasher
from skll.data import FeatureSet
from skll.data.dict_vectorizer import DictVectorizer

class Reader(object):
    
    def __init__(self, path_or_list, quiet=True, ids_to_floats=False,
                 label_col='y', id_col='id', class_map=None, sparse=True,
                 feature_hasher=False, num_features=None):
        super(Reader, self).__init__()
        self.path_or_list = path_or_list
        self.quiet = quiet
        self.ids_to_floats = ids_to_floats
        self.label_col = label_col
        self.id_col = id_col
        self.class_map = class_map
        self._progress_msg = ''
        if feature_hasher:
            self.vectorizer = FeatureHasher(n_features=num_features)
        else:
            self.vectorizer = DictVectorizer(sparse=sparse)
    @classmethod
    def for_path(cls, path_or_list, **kwargs):
                if not isinstance(path_or_list, string_types):
            return DictListReader(path_or_list)
        else:
                        ext = '.' + path_or_list.rsplit('.', 1)[-1].lower()
            if ext not in EXT_TO_READER:
                raise ValueError(('Example files must be in either .arff, '
                                  '.csv, .jsonlines, .megam, .ndj, or .tsv '
                                  'format. You specified: '
                                  '{}').format(path_or_list))
        return EXT_TO_READER[ext](path_or_list, **kwargs)
    def _sub_read(self, f):
                raise NotImplementedError
    def _print_progress(self, progress_num, end="\r"):
                        if not self.quiet:
            print("{}{:>15}".format(self._progress_msg, progress_num),
                  end=end, file=sys.stderr)
            sys.stderr.flush()
    def read(self):
                        logger = logging.getLogger(__name__)
        logger.debug('Path: %s', self.path_or_list)
        if not self.quiet:
            self._progress_msg = "Loading {}...".format(self.path_or_list)
            print(self._progress_msg, end="\r", file=sys.stderr)
            sys.stderr.flush()
                ids = []
        labels = []
        with open(self.path_or_list, 'r' if PY3 else 'rb') as f:
            for ex_num, (id_, class_, _) in enumerate(self._sub_read(f), start=1):
                                if self.ids_to_floats:
                    try:
                        id_ = float(id_)
                    except ValueError:
                        raise ValueError(('You set ids_to_floats to true,'
                                          ' but ID {} could not be '
                                          'converted to float in '
                                          '{}').format(id_,
                                                       self.path_or_list))
                ids.append(id_)
                labels.append(class_)
                if ex_num % 100 == 0:
                    self._print_progress(ex_num)
            self._print_progress(ex_num)
                total = ex_num
                ids = np.array(ids)
        labels = np.array(labels)
        def feat_dict_generator():
            with open(self.path_or_list, 'r' if PY3 else 'rb') as f:
                for ex_num, (_, _, feat_dict) in enumerate(self._sub_read(f)):
                    yield feat_dict
                    if ex_num % 100 == 0:
                        self._print_progress('{:.8}%'.format(100 * ((ex_num /
                                                                    total))))
                self._print_progress("100%")
                features = self.vectorizer.fit_transform(feat_dict_generator())
                self._print_progress("done", end="\n")
                assert ids.shape[0] == labels.shape[0] == features.shape[0]
        if ids.shape[0] != len(set(ids)):
            raise ValueError('The example IDs are not unique in %s.' %
                             self.path_or_list)
        return FeatureSet(self.path_or_list, ids, labels=labels,
                          features=features, vectorizer=self.vectorizer)

class DictListReader(Reader):
    
    def read(self):
        ids = []
        labels = []
        feat_dicts = []
        for example_num, example in enumerate(self.path_or_list):
            curr_id = str(example.get("id",
                                      "EXAMPLE_{}".format(example_num)))
            if self.ids_to_floats:
                try:
                    curr_id = float(curr_id)
                except ValueError:
                    raise ValueError(('You set ids_to_floats to true,' +
                                      ' but ID {} could not be ' +
                                      'converted to float in ' +
                                      '{}').format(curr_id, example))
            class_name = (safe_float(example['y'],
                                     replace_dict=self.class_map)
                          if 'y' in example else None)
            example = example['x']
                        if self.ids_to_floats:
                try:
                    curr_id = float(curr_id)
                except ValueError:
                    raise ValueError(('You set ids_to_floats to true, but ID '
                                      '{} could not be converted to float in '
                                      '{}').format(curr_id, self.path_or_list))
            ids.append(curr_id)
            labels.append(class_name)
            feat_dicts.append(example)
                        if example_num % 100 == 0:
                self._print_progress(example_num)
                ids = np.array(ids)
        labels = np.array(labels)
        features = self.vectorizer.fit_transform(feat_dicts)
        return FeatureSet('converted', ids, labels=labels,
                          features=features, vectorizer=self.vectorizer)

class NDJReader(Reader):
    
    def _sub_read(self, f):
        for example_num, line in enumerate(f):
                        line = line.strip()
                        if line.startswith('//') or not line:
                continue
                        example = json.loads(line)
                                    curr_id = str(example.get("id",
                                      "EXAMPLE_{}".format(example_num)))
            class_name = (safe_float(example['y'],
                                     replace_dict=self.class_map)
                          if 'y' in example else None)
            example = example["x"]
            if self.ids_to_floats:
                try:
                    curr_id = float(curr_id)
                except ValueError:
                    raise ValueError(('You set ids_to_floats to true, but' +
                                      ' ID {} could not be converted to ' +
                                      'float').format(curr_id))
            yield curr_id, class_name, example

class MegaMReader(Reader):
    
    def _sub_read(self, f):
        example_num = 0
        curr_id = 'EXAMPLE_0'
        for line in f:
                        if not isinstance(line, text_type):
                line = UnicodeDammit(line, ['utf-8',
                                            'windows-1252']).unicode_markup
            line = line.strip()
                        if line.startswith('                curr_id = line[1:].strip()
            elif line and line not in ['TRAIN', 'TEST', 'DEV']:
                split_line = line.split()
                num_cols = len(split_line)
                del line
                                if num_cols == 1:
                    class_name = safe_float(split_line[0],
                                            replace_dict=self.class_map)
                    field_pairs = []
                                elif num_cols % 2 == 1:
                    class_name = safe_float(split_line[0],
                                            replace_dict=self.class_map)
                    field_pairs = split_line[1:]
                                elif num_cols % 2 == 0:
                    class_name = None
                    field_pairs = split_line
                curr_info_dict = {}
                if len(field_pairs) > 0:
                                        field_names = islice(field_pairs, 0, None, 2)
                                                            field_values = (safe_float(val) for val in
                                    islice(field_pairs, 1, None, 2))
                                        curr_info_dict.update(zip(field_names, field_values))
                    if len(curr_info_dict) != len(field_pairs) / 2:
                        raise ValueError(('There are duplicate feature ' +
                                          'names in {} for example ' +
                                          '{}.').format(self.path_or_list,
                                                        curr_id))
                yield curr_id, class_name, curr_info_dict
                                                example_num += 1
                curr_id = 'EXAMPLE_{}'.format(example_num)

class LibSVMReader(Reader):
    
    line_regex = re.compile(r'^(?P<label_num>[^ ]+)\s+(?P<features>[^                            r'(?P<comments>                            r'(?P<label_map>[^|]+)\s*\|\s*'
                            r'(?P<feat_map>.*)\s*)?$', flags=re.UNICODE)
    LIBSVM_REPLACE_DICT = {'\u2236': ':',
                           '\uFF03': '                           '\u2002': ' ',
                           '\ua78a': '=',
                           '\u2223': '|'}
    @staticmethod
    def _pair_to_tuple(pair, feat_map):
                name, value = pair.split(':')
        if feat_map is not None:
            name = feat_map[name]
        value = safe_float(value)
        return (name, value)
    def _sub_read(self, f):
        for example_num, line in enumerate(f):
            curr_id = ''
                        if isinstance(line, bytes):
                line = UnicodeDammit(line, ['utf-8',
                                            'windows-1252']).unicode_markup
            match = self.line_regex.search(line.strip())
            if not match:
                raise ValueError('Line does not look like valid libsvm format'
                                 '\n{}'.format(line))
                        if match.group('comments') is not None:
                                if match.group('feat_map'):
                    feat_map = {}
                    for pair in match.group('feat_map').split():
                        number, name = pair.split('=')
                        for orig, replacement in \
                                LibSVMReader.LIBSVM_REPLACE_DICT.items():
                            name = name.replace(orig, replacement)
                        feat_map[number] = name
                else:
                    feat_map = None
                                if match.group('label_map'):
                    label_map = dict(pair.split('=') for pair in
                                     match.group('label_map').strip().split())
                else:
                    label_map = None
                curr_id = match.group('example_id').strip()
            if not curr_id:
                curr_id = 'EXAMPLE_{}'.format(example_num)
            class_num = match.group('label_num')
                        if label_map:
                class_name = label_map[class_num]
            else:
                class_name = class_num
            class_name = safe_float(class_name,
                                    replace_dict=self.class_map)
            curr_info_dict = dict(self._pair_to_tuple(pair, feat_map) for pair
                                  in match.group('features').strip().split())
            yield curr_id, class_name, curr_info_dict

class DelimitedReader(Reader):
    
    def __init__(self, path_or_list, **kwargs):
        self.dialect = kwargs.pop('dialect', 'excel-tab')
        super(DelimitedReader, self).__init__(path_or_list, **kwargs)
    def _sub_read(self, f):
        reader = DictReader(f, dialect=self.dialect)
        for example_num, row in enumerate(reader):
            if self.label_col is not None and self.label_col in row:
                class_name = safe_float(row[self.label_col],
                                        replace_dict=self.class_map)
                del row[self.label_col]
            else:
                class_name = None
            if self.id_col not in row:
                curr_id = "EXAMPLE_{}".format(example_num)
            else:
                curr_id = row[self.id_col]
                del row[self.id_col]
                                                            columns_to_delete = []
            if PY2:
                columns_to_convert_to_unicode = []
            for fname, fval in iteritems(row):
                fval_float = safe_float(fval)
                                if fval_float:
                    row[fname] = fval_float
                    if PY2:
                        columns_to_convert_to_unicode.append(fname)
                else:
                    columns_to_delete.append(fname)
                        for cname in columns_to_delete:
                del row[cname]
                                    if PY2:
                for cname in columns_to_convert_to_unicode:
                    fval = row[cname]
                    del row[cname]
                    row[cname.decode('utf-8')] = fval
                if not self.ids_to_floats:
                    curr_id = curr_id.decode('utf-8')
            yield curr_id, class_name, row

class CSVReader(DelimitedReader):
    
    def __init__(self, path_or_list, **kwargs):
        kwargs['dialect'] = 'excel'
        super(CSVReader, self).__init__(path_or_list, **kwargs)

class ARFFReader(DelimitedReader):
    
    def __init__(self, path_or_list, **kwargs):
        kwargs['dialect'] = 'arff'
        super(ARFFReader, self).__init__(path_or_list, **kwargs)
        self.relation = ''
        self.regression = False
    @staticmethod
    def split_with_quotes(s, delimiter=' ', quote_char="'", escape_char='\\'):
                if PY2:
            delimiter = delimiter.encode()
            quote_char = quote_char.encode()
            escape_char = escape_char.encode()
        return next(csv.reader([s], delimiter=delimiter, quotechar=quote_char,
                               escapechar=escape_char))
    def _sub_read(self, f):
        field_names = []
                for line in f:
                        if not isinstance(line, text_type):
                decoded_line = UnicodeDammit(line,
                                             ['utf-8',
                                              'windows-1252']).unicode_markup
            else:
                decoded_line = line
            line = decoded_line.strip()
                        if line:
                                                split_header = self.split_with_quotes(line)
                row_type = split_header[0].lower()
                if row_type == '@attribute':
                                        field_name = split_header[1]
                    field_names.append(field_name)
                                        if field_name == self.label_col:
                        self.regression = (len(split_header) > 2 and
                                           split_header[2] == 'numeric')
                                elif row_type == '@relation':
                    self.relation = split_header[1]
                                elif row_type == '@data':
                    break
                
                if PY2:
            io_type = BytesIO
        else:
            io_type = StringIO
        with io_type() as field_buffer:
            csv.writer(field_buffer, dialect='arff').writerow(field_names)
            field_str = field_buffer.getvalue()
                        if self.label_col != field_names[-1]:
            self.label_col = None
                return super(ARFFReader, self)._sub_read(chain([field_str], f))

class TSVReader(DelimitedReader):
    
    def __init__(self, path_or_list, **kwargs):
        kwargs['dialect'] = 'excel-tab'
        super(TSVReader, self).__init__(path_or_list, **kwargs)

def safe_float(text, replace_dict=None):
    
        text = text_type(text)
    if replace_dict is not None:
        if text in replace_dict:
            text = replace_dict[text]
        else:
            logging.getLogger(__name__).warning('Encountered value that was '
                                                'not in replacement '
                                                'dictionary (e.g., class_map):'
                                                ' {}'.format(text))
    try:
        return int(text)
    except ValueError:
        try:
            return float(text)
        except ValueError:
            return text.decode('utf-8') if PY2 else text
        except TypeError:
            return 0.0
    except TypeError:
        return 0

EXT_TO_READER = {".arff": ARFFReader,
                 ".csv": CSVReader,
                 ".jsonlines": NDJReader,
                 ".libsvm": LibSVMReader,
                 ".megam": MegaMReader,
                 '.ndj': NDJReader,
                 ".tsv": TSVReader}
from __future__ import absolute_import, print_function, unicode_literals
import json
import logging
import os
import re
import sys
from csv import DictWriter
from decimal import Decimal
from io import open
import numpy as np
from six import iteritems, PY2, string_types, text_type
from six.moves import map
from sklearn.feature_extraction import FeatureHasher

class Writer(object):
    
    def __init__(self, path, feature_set, **kwargs):
        super(Writer, self).__init__()
        self.requires_binary = kwargs.pop('requires_binary', False)
        self.quiet = kwargs.pop('quiet', True)
        self.path = path
        self.feat_set = feature_set
        self.subsets = kwargs.pop('subsets', None)
                        self.root, self.ext = re.search(r'^(.*)(\.[^.]*)$', path).groups()
        self._progress_msg = ''
        if kwargs:
            raise ValueError('Passed extra keyword arguments to '
                             'Writer constructor: {}'.format(kwargs))
    @classmethod
    def for_path(cls, path, feature_set, **kwargs):
                        ext = '.' + path.rsplit('.', 1)[-1].lower()
        return EXT_TO_WRITER[ext](path, feature_set, **kwargs)
    def write(self):
                        logger = logging.getLogger(__name__)
        if isinstance(self.feat_set.vectorizer, FeatureHasher):
            raise ValueError('Writer cannot write sets that use'
                             'FeatureHasher for vectorization.')
                if self.subsets is None:
            self._write_subset(self.path, None)
                else:
            for subset_name, filter_features in iteritems(self.subsets):
                logger.debug('Subset (%s) features: %s', subset_name,
                             filter_features)
                sub_path = os.path.join(self.root, '{}{}'.format(subset_name,
                                                                 self.ext))
                self._write_subset(sub_path, set(filter_features))
    def _write_subset(self, sub_path, filter_features):
                        logger = logging.getLogger(__name__)
        logger.debug('sub_path: %s', sub_path)
        logger.debug('feature_set: %s', self.feat_set.name)
        logger.debug('filter_features: %s', filter_features)
        if not self.quiet:
            self._progress_msg = "Writing {}...".format(sub_path)
            print(self._progress_msg, end="\r", file=sys.stderr)
            sys.stderr.flush()
                filtered_set = (self.feat_set.filtered_iter(features=filter_features)
                        if filter_features is not None else self.feat_set)
                file_mode = 'wb' if (self.requires_binary and PY2) else 'w'
        with open(sub_path, file_mode) as output_file:
                        self._write_header(filtered_set, output_file, filter_features)
                        for ex_num, (id_, label_, feat_dict) in enumerate(filtered_set):
                self._write_line(id_, label_, feat_dict, output_file)
                if not self.quiet and ex_num % 100 == 0:
                    print("{}{:>15}".format(self._progress_msg, ex_num),
                          end="\r", file=sys.stderr)
                    sys.stderr.flush()
            if not self.quiet:
                print("{}{:<15}".format(self._progress_msg, "done"),
                      file=sys.stderr)
                sys.stderr.flush()
    def _write_header(self, feature_set, output_file, filter_features):
                pass
    def _write_line(self, id_, label_, feat_dict, output_file):
                raise NotImplementedError

class DelimitedFileWriter(Writer):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['requires_binary'] = True
        self.dialect = kwargs.pop('dialect', 'excel-tab')
        self.label_col = kwargs.pop('label_col', 'y')
        self.id_col = kwargs.pop('id_col', 'id')
        super(DelimitedFileWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None
    def _get_fieldnames(self, filter_features):
                        if filter_features is not None:
            fieldnames = {feat_name for feat_name in
                          self.feat_set.vectorizer.get_feature_names() if
                          (feat_name in filter_features or
                           feat_name.split('=', 1)[0] in filter_features)}
        else:
            fieldnames = set(self.feat_set.vectorizer.get_feature_names())
        fieldnames.add(self.id_col)
        if self.feat_set.has_labels:
            fieldnames.add(self.label_col)
        return sorted(fieldnames)
    def _write_header(self, feature_set, output_file, filter_features):
                        self._dict_writer = DictWriter(output_file,
                                       self._get_fieldnames(filter_features),
                                       restval=0, dialect=self.dialect)
                self._dict_writer.writeheader()
    def _write_line(self, id_, label_, feat_dict, output_file):
                        if self.label_col not in feat_dict:
            if self.feat_set.has_labels:
                feat_dict[self.label_col] = label_
        else:
            raise ValueError(('Class column name "{}" already used as feature '
                              'name.').format(self.label_col))
                if self.id_col not in feat_dict:
            feat_dict[self.id_col] = id_
        else:
            raise ValueError('ID column name "{}" already used as feature '
                             'name.'.format(self.id_col))
                self._dict_writer.writerow(feat_dict)

class CSVWriter(DelimitedFileWriter):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['dialect'] = 'excel'
        super(CSVWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None

class TSVWriter(DelimitedFileWriter):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['dialect'] = 'excel-tab'
        super(TSVWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None

class ARFFWriter(DelimitedFileWriter):
    
    def __init__(self, path, feature_set, **kwargs):
        self.relation = kwargs.pop('relation', 'skll_relation')
        self.regression = kwargs.pop('regression', False)
        kwargs['dialect'] = 'arff'
        super(ARFFWriter, self).__init__(path, feature_set, **kwargs)
        self._dict_writer = None
    def _write_header(self, feature_set, output_file, filter_features):
                fieldnames = self._get_fieldnames(filter_features)
        if self.label_col in fieldnames:
            fieldnames.remove(self.label_col)
                print("@relation '{}'\n".format(self.relation), file=output_file)
                for field in fieldnames:
            print("@attribute '{}' numeric".format(field.replace('\\', '\\\\')
                                                   .replace("'", "\\'")),
                  file=output_file)
                if self.regression:
            print("@attribute {} numeric".format(self.label_col),
                  file=output_file)
        else:
            if self.feat_set.has_labels:
                print("@attribute {} ".format(self.label_col) +
                      "{" + ','.join(map(str,
                                         sorted(set(self.feat_set.labels)))) +
                      "}", file=output_file)
        fieldnames.append(self.label_col)
                        self._dict_writer = DictWriter(output_file, fieldnames, restval=0,
                                       extrasaction='ignore', dialect='arff')
                print("\n@data", file=output_file)

class MegaMWriter(Writer):
        @staticmethod
    def _replace_non_ascii(line):
                char_list = []
        for char in line:
            char_num = ord(char)
            char_list.append(
                '<U{}>'.format(char_num) if char_num > 127 else char)
        return ''.join(char_list)
    def _write_line(self, id_, label_, feat_dict, output_file):
                        print('        if self.feat_set.has_labels:
            print('{}'.format(label_), end='\t', file=output_file)
        print(self._replace_non_ascii(' '.join(('{} {}'.format(field,
                                                               value) for
                                                field, value in
                                                sorted(feat_dict.items()) if
                                                Decimal(value) != 0))),
              file=output_file)

class NDJWriter(Writer):
    
    def __init__(self, path, feature_set, **kwargs):
        kwargs['requires_binary'] = True
        super(NDJWriter, self).__init__(path, feature_set, **kwargs)
    def _write_line(self, id_, label_, feat_dict, output_file):
                example_dict = {}
                if self.feat_set.has_labels:
            example_dict['y'] = np.asscalar(label_)
        example_dict['id'] = np.asscalar(id_)
        example_dict["x"] = feat_dict
        print(json.dumps(example_dict, sort_keys=True), file=output_file)

class LibSVMWriter(Writer):
    
    LIBSVM_REPLACE_DICT = {':': '\u2236',
                           '                           ' ': '\u2002',
                           '=': '\ua78a',
                           '|': '\u2223'}
    def __init__(self, path, feature_set, **kwargs):
        self.label_map = kwargs.pop('label_map', None)
        super(LibSVMWriter, self).__init__(path, feature_set, **kwargs)
        if self.label_map is None:
            self.label_map = {}
            if feature_set.has_labels:
                self.label_map = {label: num for num, label in
                                  enumerate(sorted({label for label in
                                                    feature_set.labels if
                                                    not isinstance(label,
                                                                   (int,
                                                                    float))}))}
                        self.label_map[None] = '00000'
    @staticmethod
    def _sanitize(name):
                if isinstance(name, string_types):
            for orig, replacement in LibSVMWriter.LIBSVM_REPLACE_DICT.items():
                name = name.replace(orig, replacement)
        return name
    def _write_line(self, id_, label_, feat_dict, output_file):
                field_values = sorted([(self.feat_set.vectorizer.vocabulary_[field] +
                                1, value) for field, value in
                               iteritems(feat_dict) if Decimal(value) != 0])
                if label_ in self.label_map:
            print('{}'.format(self.label_map[label_]), end=' ',
                  file=output_file)
        else:
            print('{}'.format(label_), end=' ', file=output_file)
                print(' '.join(('{}:{}'.format(field, value) for field, value in
                        field_values)), end=' ', file=output_file)
                print('        print(self._sanitize('{}'.format(id_)), end='',
              file=output_file)
        print(' |', end=' ', file=output_file)
        if (PY2 and self.feat_set.has_labels and isinstance(label_,
                                                            text_type)):
            label_ = label_.encode('utf-8')
        if label_ in self.label_map:
            print('%s=%s' % (self._sanitize(self.label_map[label_]),
                             self._sanitize(label_)),
                  end=' | ', file=output_file)
        else:
            print(' |', end=' ', file=output_file)
        line = ' '.join(('%s=%s' % (self.feat_set.vectorizer.vocabulary_[field]
                                    + 1, self._sanitize(field)) for
                         field, value in feat_dict.items() if
                         Decimal(value) != 0))
        print(line, file=output_file)

EXT_TO_WRITER = {".arff": ARFFWriter,
                 ".csv": CSVWriter,
                 ".jsonlines": NDJWriter,
                 ".libsvm": LibSVMWriter,
                 ".megam": MegaMWriter,
                 '.ndj': NDJWriter,
                 ".tsv": TSVWriter}
from __future__ import absolute_import, print_function, unicode_literals
import csv
from six import PY2
from .featureset import FeatureSet
from .readers import (ARFFReader, CSVReader, LibSVMReader, MegaMReader,
                      NDJReader, TSVReader, safe_float, Reader)
from .writers import (ARFFWriter, DelimitedFileWriter, Writer,
                      LibSVMWriter, MegaMWriter, NDJWriter)

if PY2:
    csv.register_dialect('arff', delimiter=b',', quotechar=b"'",
                         escapechar=b'\\', doublequote=False,
                         lineterminator=b'\n', skipinitialspace=True)
else:
    csv.register_dialect('arff', delimiter=',', quotechar="'",
                         escapechar='\\', doublequote=False,
                         lineterminator='\n', skipinitialspace=True)

__all__ = ['Reader', 'safe_float', 'FeatureSet', 'ARFFReader',
           'CSVReader', 'LibSVMReader', 'MegaMReader', 'NDJReader',
           'TSVReader', 'ARFFWriter', 'DelimitedFileWriter', 'LibSVMWriter',
           'MegaMWriter', 'NDJWriter', 'Writer']
from __future__ import print_function, unicode_literals
import argparse
import csv
import logging
from skll.data import Reader, safe_float
from skll.metrics import use_score_func
from skll.version import __version__

def compute_eval_from_predictions(examples_file, predictions_file,
                                  metric_names):
    
        data = Reader.for_path(examples_file).read()
    gold = dict(zip(data.ids, data.labels))
        pred = {}
    with open(predictions_file) as pred_file:
        reader = csv.reader(pred_file, dialect=csv.excel_tab)
        next(reader)          for row in reader:
            pred[row[0]] = safe_float(row[1])
            if set(gold.keys()) != set(pred.keys()):
        raise ValueError('The example and prediction IDs do not match.')
    example_ids = sorted(gold.keys())
    res = {}
    for metric_name in metric_names:
        score = use_score_func(metric_name,
                               [gold[ex_id] for ex_id in example_ids],
                               [pred[ex_id] for ex_id in example_ids])
        res[metric_name] = score
    return res

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Computes evaluation metrics from prediction files after \
                     you have run an experiment.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('examples_file',
                        help='SKLL input file with labeled examples')
    parser.add_argument('predictions_file',
                        help='file with predictions from SKLL')
    parser.add_argument('metric_names',
                        help='metrics to compute',
                        nargs='+')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    scores = compute_eval_from_predictions(args.examples_file,
                                           args.predictions_file,
                                           args.metric_names)
    for metric_name in args.metric_names:
        print("{}\t{}\t{}".format(scores[metric_name],
                                  metric_name, args.predictions_file))

if __name__ == '__main__':
    main()
'''
Script that filters a given feature file to remove unwanted features, labels,
or IDs.
:author: Dan Blanchard (dblanchard@ets.org)
:date: November 2014
'''
from __future__ import print_function, unicode_literals
import argparse
import logging
import os
import sys
from skll.data.readers import EXT_TO_READER
from skll.data.writers import ARFFWriter, DelimitedFileWriter, EXT_TO_WRITER
from skll.version import __version__

def main(argv=None):
    '''
    Handles command line arguments and gets things started.
    :param argv: List of arguments, as if specified on the command-line.
                 If None, ``sys.argv[1:]`` is used instead.
    :type argv: list of str
    '''
        parser = argparse.ArgumentParser(
        description="Takes an input feature file and removes any instances or\
                     features that do not match the specified patterns.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('infile',
                        help='input feature file (ends in .arff, .csv, \
                              .jsonlines, .megam, .ndj, or .tsv)')
    parser.add_argument('outfile',
                        help='output feature file (must have same extension as\
                              input file)')
    parser.add_argument('-f', '--feature',
                        help='A feature in the feature file you would like to \
                              keep.  If unspecified, no features are removed.',
                        nargs='*')
    parser.add_argument('-I', '--id',
                        help='An instance ID in the feature file you would \
                              like to keep.  If unspecified, no instances are \
                              removed based on their IDs.',
                        nargs='*')
    parser.add_argument('--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-i', '--inverse',
                        help='Instead of keeping features and/or examples in \
                              lists, remove them.',
                        action='store_true')
    parser.add_argument('-L', '--label',
                        help='A label in the feature file you would like to \
                              keep.  If unspecified, no instances are removed \
                              based on their labels.',
                        nargs='*')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the class \
                              labels in ARFF, CSV, or TSV files. For ARFF \
                              files, this must be the final column to count as\
                              the label.',
                        default='y')
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - '
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        valid_extensions = {ext for ext in EXT_TO_READER if ext != '.libsvm'}
        input_extension = os.path.splitext(args.infile)[1].lower()
    output_extension = os.path.splitext(args.outfile)[1].lower()
    if input_extension == '.libsvm':
        logger.error('Cannot filter LibSVM files.  Please use skll_convert to '
                     'convert to a different datatype first.')
        sys.exit(1)
    if input_extension not in valid_extensions:
        logger.error(('Input file must be in either .arff, .csv, .jsonlines, '
                      '.megam, .ndj, or .tsv format. You specified: '
                      '{}').format(input_extension))
        sys.exit(1)
    if output_extension != input_extension:
        logger.error(('Output file must be in the same format as the input '
                      'file.  You specified: {}').format(output_extension))
        sys.exit(1)
        reader = EXT_TO_READER[input_extension](args.infile, quiet=args.quiet,
                                            label_col=args.label_col,
                                            id_col=args.id_col)
    feature_set = reader.read()
        feature_set.filter(ids=args.id, labels=args.label, features=args.feature,
                       inverse=args.inverse)
        writer_type = EXT_TO_WRITER[input_extension]
    writer_args = {'quiet': args.quiet}
    if writer_type is DelimitedFileWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
    elif writer_type is ARFFWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
        writer_args['regression'] = reader.regression
        writer_args['relation'] = reader.relation
    writer = writer_type(args.outfile, feature_set, **writer_args)
    writer.write()

if __name__ == '__main__':
    main()
from __future__ import absolute_import, print_function, unicode_literals
import argparse
import logging
import os
from skll.data.readers import EXT_TO_READER
from skll.learner import Learner
from skll.version import __version__

class Predictor(object):
    
    def __init__(self, model_path, threshold=None, positive_label=1):
                self._learner = Learner.from_file(model_path)
        self._pos_index = positive_label
        self.threshold = threshold
    def predict(self, data):
                        preds = self._learner.predict(data)
        preds = preds.tolist()
        if self._learner.probability:
            if self.threshold is None:
                return [pred[self._pos_index] for pred in preds]
            else:
                return [int(pred[self._pos_index] >= self.threshold)
                        for pred in preds]
        elif self._learner.model._estimator_type == 'regressor':
            return preds
        else:
            return [self._learner.label_list[pred if isinstance(pred, int) else
                                             int(pred[0])] for pred in preds]

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Loads a trained model and outputs predictions based \
                     on input feature files.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        conflict_handler='resolve')
    parser.add_argument('model_file',
                        help='Model file to load and use for generating \
                              predictions.')
    parser.add_argument('input_file',
                        help='A csv file, json file, or megam file \
                              (with or without the label column), \
                              with the appropriate suffix.',
                        nargs='+')
    parser.add_argument('-i', '--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the labels\
                              in ARFF, CSV, or TSV files. For ARFF files, this\
                              must be the final column to count as the label.',
                        default='y')
    parser.add_argument('-p', '--positive_label',
                        help="If the model is only being used to predict the \
                              probability of a particular label, this \
                              specifies the index of the label we're \
                              predicting. 1 = second label, which is default \
                              for binary classification. Keep in mind that \
                              labels are sorted lexicographically.",
                        default=1, type=int)
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('-t', '--threshold',
                        help="If the model we're using is generating \
                              probabilities of the positive label, return 1 \
                              if it meets/exceeds the given threshold and 0 \
                              otherwise.",
                        type=float)
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        predictor = Predictor(args.model_file,
                          positive_label=args.positive_label,
                          threshold=args.threshold)
        for input_file in args.input_file:
                input_extension = os.path.splitext(input_file)[1].lower()
        if input_extension not in EXT_TO_READER:
            logger.error(('Input file must be in either .arff, .csv, '
                          '.jsonlines, .libsvm, .megam, .ndj, or .tsv format. '
                          ' Skipping file {}').format(input_file))
            continue
        else:
                        reader = EXT_TO_READER[input_extension](input_file,
                                                    quiet=args.quiet,
                                                    label_col=args.label_col,
                                                    id_col=args.id_col)
            feature_set = reader.read()
            for pred in predictor.predict(feature_set):
                print(pred)

if __name__ == '__main__':
    main()
'''
Script that joins a bunch of feature files together to create one file.
:author: Dan Blanchard (dblanchard@ets.org)
:date: November 2014
'''
from __future__ import print_function, unicode_literals
import argparse
import logging
import os
import sys
from skll.data.readers import EXT_TO_READER
from skll.data.writers import ARFFWriter, DelimitedFileWriter, EXT_TO_WRITER
from skll.version import __version__

def main(argv=None):
    '''
    Handles command line arguments and gets things started.
    :param argv: List of arguments, as if specified on the command-line.
                 If None, ``sys.argv[1:]`` is used instead.
    :type argv: list of str
    '''
        parser = argparse.ArgumentParser(
        description="Joins multiple input feature files together into one \
                     file.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('infile',
                        help='input feature file (ends in .arff, .csv, \
                              .jsonlines, .megam, .ndj, or .tsv)',
                        nargs='+')
    parser.add_argument('outfile',
                        help='output feature file')
    parser.add_argument('-i', '--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the class \
                              labels in ARFF, CSV, or TSV files. For ARFF \
                              files, this must be the final column to count as\
                              the label.',
                        default='y')
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - '
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        valid_extensions = {ext for ext in EXT_TO_READER if ext != '.libsvm'}
        input_extension_set = {os.path.splitext(inf)[1].lower() for inf in
                           args.infile}
    output_extension = os.path.splitext(args.outfile)[1].lower()
        if len(input_extension_set) > 1:
        logger.error('All input files must be in the same format.')
        sys.exit(1)
    input_extension = list(input_extension_set)[0]
    if input_extension not in valid_extensions:
        logger.error(('Input files must be in either .arff, .csv, .jsonlines, '
                      '.megam, .ndj, or .tsv format. You specified: '
                      '{}').format(input_extension))
        sys.exit(1)
    if output_extension != input_extension:
        logger.error(('Output file must be in the same format as the input '
                      'file.  You specified: {}').format(output_extension))
        sys.exit(1)
        merged_set = None
    for infile in args.infile:
        reader = EXT_TO_READER[input_extension](infile, quiet=args.quiet,
                                                label_col=args.label_col,
                                                id_col=args.id_col)
        fs = reader.read()
        if merged_set is None:
            merged_set = fs
        else:
            merged_set += fs
        writer_type = EXT_TO_WRITER[input_extension]
    writer_args = {'quiet': args.quiet}
    if writer_type is DelimitedFileWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
    elif writer_type is ARFFWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
        writer_args['regression'] = reader.regression
        writer_args['relation'] = reader.relation
    writer = writer_type(args.outfile, merged_set, **writer_args)
    writer.write()

if __name__ == '__main__':
    main()
from __future__ import print_function, unicode_literals
import argparse
import logging
import sys
from six import iteritems
import numpy as np
from skll import Learner
from skll.version import __version__

def main(argv=None):
        parser = argparse.ArgumentParser(description="Prints out the weights of a \
                                                  given model.",
                                     conflict_handler='resolve',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('model_file', help='model file to load')
    parser.add_argument('--k',
                        help='number of top features to print (0 for all)',
                        type=int, default=50)
    parser.add_argument('--sign',
                        choices=['positive', 'negative', 'all'],
                        default='all',
                        help='show only positive, only negative, ' +
                             'or all weights')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    k = args.k if args.k > 0 else None
    learner = Learner.from_file(args.model_file)
    (weights, intercept) = learner.model_params
    weight_items = iteritems(weights)
    if args.sign == 'positive':
        weight_items = (x for x in weight_items if x[1] > 0)
    elif args.sign == 'negative':
        weight_items = (x for x in weight_items if x[1] < 0)
    if intercept is not None:
                if '_intercept_' in intercept:
                        if isinstance(intercept['_intercept_'], np.ndarray):
                intercept_list = ["%.12f" % i for i in intercept['_intercept_']]
                print("intercept = {}".format(intercept_list))
            else:
                print("intercept = {:.12f}".format(intercept['_intercept_']))
        else:
            print("== intercept values ==")
            for (label, val) in intercept.items():
                print("{:.12f}\t{}".format(val, label))
        print()
    print("Number of nonzero features:", len(weights), file=sys.stderr)
    for feat, val in sorted(weight_items, key=lambda x: -abs(x[1]))[:k]:
        print("{:.12f}\t{}".format(val, feat))

if __name__ == '__main__':
    main()

from __future__ import print_function, unicode_literals
import argparse
import logging
from skll.experiments import run_configuration
from skll.version import __version__

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Runs the scikit-learn experiments in a given config file.\
                     If Grid Map is installed, jobs will automatically be \
                     created and run on a DRMAA-compatible cluster.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        conflict_handler='resolve')
    parser.add_argument('config_file',
                        help='Configuration file describing the sklearn task\
                              to run.',
                        nargs='+')
    parser.add_argument('-a', '--ablation',
                        help='Runs an ablation study where repeated \
                              experiments are conducted where the specified \
                              number of features in each featureset in the \
                              configuration file are held out.',
                        type=int, default=0,
                        metavar='NUM_FEATURES')
    parser.add_argument('-A', '--ablation_all',
                        help='Runs an ablation study where repeated \
                              experiments are conducted with all combinations \
                              of features in each featureset in the \
                              configuration file. Overrides --ablation \
                              setting.',
                        action='store_true')
    parser.add_argument('-k', '--keep_models',
                        help='If trained models already exists, re-use them\
                              instead of overwriting them.',
                        action='store_true')
    parser.add_argument('-l', '--local',
                        help='Do not use the Grid Engine for running jobs and\
                              just run everything sequentially on the local \
                              machine. This is for debugging.',
                        action='store_true')
    parser.add_argument('-m', '--machines',
                        help="comma-separated list of machines to add to\
                              gridmap's whitelist (if not specified, all\
                              available machines are used). Note that full \
                              names must be specified, e.g., \
                              \"nlp.research.ets.org\"",
                        default=None)
    parser.add_argument('-q', '--queue',
                        help="Use this queue for gridmap.",
                        default='all.q')
    parser.add_argument('-r', '--resume',
                        help='If result files already exist for an experiment, \
                              do not overwrite them. This is very useful when \
                              doing a large ablation experiment and part of it \
                              crashes.',
                        action='store_true')
    parser.add_argument('-v', '--verbose',
                        help='Print more status information. For every ' +
                             'additional time this flag is specified, ' +
                             'output gets more verbose.',
                        default=0, action='count')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        log_level = max(logging.WARNING - (args.verbose * 10), logging.DEBUG)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'), level=log_level)
    machines = None
    if args.machines:
        machines = args.machines.split(',')
    ablation = args.ablation
    if args.ablation_all:
        ablation = None
        for config_file in args.config_file:
        run_configuration(config_file, local=args.local, overwrite=not
                          args.keep_models, queue=args.queue, hosts=machines,
                          ablation=ablation, resume=args.resume)

if __name__ == '__main__':
    main()
from __future__ import print_function, unicode_literals
import argparse
import logging
import os
import sys
from bs4 import UnicodeDammit
from six import PY2
from skll.data.dict_vectorizer import DictVectorizer
from skll.data.readers import EXT_TO_READER
from skll.data.writers import (ARFFWriter, DelimitedFileWriter, LibSVMWriter,
                               EXT_TO_WRITER)
from skll.version import __version__

def _pair_to_dict_tuple(pair):
        number, name = pair.split('=')
    if PY2:
        name = name.encode('utf-8')
    number = int(number)
    return (name, number)

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Takes an input feature file and converts it to another \
                     format. Formats are determined automatically from file \
                     extensions.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('infile',
                        help='input feature file (ends in .arff, .csv, \
                              .jsonlines, .libsvm, .megam, .ndj, or .tsv)')
    parser.add_argument('outfile',
                        help='output feature file (ends in .arff, .csv, \
                              .jsonlines, .libsvm, .megam, .ndj, or .tsv)')
    parser.add_argument('-i', '--id_col',
                        help='Name of the column which contains the instance \
                              IDs in ARFF, CSV, or TSV files.',
                        default='id')
    parser.add_argument('-l', '--label_col',
                        help='Name of the column which contains the class \
                              labels in ARFF, CSV, or TSV files. For ARFF \
                              files, this must be the final column to count as\
                              the label.',
                        default='y')
    parser.add_argument('-q', '--quiet',
                        help='Suppress printing of "Loading..." messages.',
                        action='store_true')
    parser.add_argument('--arff_regression',
                        help='Create ARFF files for regression, not \
                              classification.',
                        action='store_true')
    parser.add_argument('--arff_relation',
                        help='Relation name to use for ARFF file.',
                        default='skll_relation')
    parser.add_argument('--reuse_libsvm_map',
                        help='If you want to output multiple files that use \
                              the same mapping from labels and features to \
                              numbers when writing libsvm files, you can \
                              specify an existing .libsvm file to reuse the \
                              mapping from.',
                        type=argparse.FileType('rb'))
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - '
                                '%(message)s'))
    logger = logging.getLogger(__name__)
        input_extension = os.path.splitext(args.infile)[1].lower()
    output_extension = os.path.splitext(args.outfile)[1].lower()
    if input_extension not in EXT_TO_READER:
        logger.error(('Input file must be in either .arff, .csv, .jsonlines, '
                      '.libsvm, .megam, .ndj, or .tsv format. You specified: '
                      '{}').format(input_extension))
        sys.exit(1)
        if args.reuse_libsvm_map and output_extension == '.libsvm':
        feat_map = {}
        label_map = {}
        for line in args.reuse_libsvm_map:
            line = UnicodeDammit(line, ['utf-8',
                                        'windows-1252']).unicode_markup
            if '                logger.error('The LibSVM file you want to reuse the map from '
                             'was not created by SKLL and does not actually '
                             'contain the necessary mapping info.')
                sys.exit(1)
            comments = line.split('            _, label_map_str, feat_map_str = comments.split('|')
            feat_map.update(_pair_to_dict_tuple(pair) for pair in
                            feat_map_str.strip().split())
            label_map.update(_pair_to_dict_tuple(pair) for pair in
                             label_map_str
                             .strip().split())
        feat_vectorizer = DictVectorizer()
        feat_vectorizer.fit([{name: 1} for name in feat_map])
        feat_vectorizer.vocabulary_ = feat_map
    else:
        feat_vectorizer = None
        label_map = None
        reader = EXT_TO_READER[input_extension](args.infile, quiet=args.quiet,
                                            label_col=args.label_col,
                                            id_col=args.id_col)
    feature_set = reader.read()
        writer_type = EXT_TO_WRITER[output_extension]
    writer_args = {'quiet': args.quiet}
    if writer_type is DelimitedFileWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
    elif writer_type is ARFFWriter:
        writer_args['label_col'] = args.label_col
        writer_args['id_col'] = args.id_col
        writer_args['regression'] = args.arff_regression
        writer_args['relation'] = args.arff_relation
    elif writer_type is LibSVMWriter:
        writer_args['label_map'] = label_map
    writer = writer_type(args.outfile, feature_set, **writer_args)
    writer.write()
if __name__ == '__main__':
    main()
from __future__ import print_function, unicode_literals
import argparse
import logging
import sys
from io import open
from skll.experiments import _write_summary_file
from skll.version import __version__

def main(argv=None):
            parser = argparse.ArgumentParser(
        description="Creates an experiment summary TSV file from a list of JSON\
                     files generated by run_experiment.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        conflict_handler='resolve')
    parser.add_argument('summary_file',
                        help='TSV file to store summary of results.')
    parser.add_argument('json_file',
                        help='JSON results file generated by run_experiment.',
                        nargs='+')
    parser.add_argument('-a', '--ablation', action='store_true', default=False,
                        help='The results files are from an ablation run.')
    parser.add_argument('--version', action='version',
                        version='%(prog)s {0}'.format(__version__))
    args = parser.parse_args(argv)
        logging.captureWarnings(True)
    logging.basicConfig(format=('%(asctime)s - %(name)s - %(levelname)s - ' +
                                '%(message)s'))
    file_mode = 'w' if sys.version_info >= (3, 0) else 'wb'
    with open(args.summary_file, file_mode) as output_file:
        _write_summary_file(args.json_file, output_file,
                            ablation=int(args.ablation))

if __name__ == '__main__':
    main()
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import re
from os.path import abspath, dirname, join
import numpy as np
from numpy.random import RandomState
from sklearn.datasets.samples_generator import (make_classification,
                                                make_regression)
from sklearn.feature_extraction import FeatureHasher
from skll.data import FeatureSet
from skll.config import _setup_config_parser
_my_dir = abspath(dirname(__file__))

def fill_in_config_paths(config_template_path):
    
    train_dir = join(_my_dir, 'train')
    test_dir = join(_my_dir, 'test')
    output_dir = join(_my_dir, 'output')
    config = _setup_config_parser(config_template_path, validate=False)
    task = config.get("General", "task")
    config.set("Input", "train_directory", train_dir)
    to_fill_in = ['log']
    if task != 'learning_curve':
      to_fill_in.append('predictions')
    if task not in ['cross_validate', 'learning_curve']:
        to_fill_in.append('models')
    if task in ['cross_validate', 'evaluate', 'learning_curve']:
        to_fill_in.append('results')
    for d in to_fill_in:
        config.set("Output", d, join(output_dir))
    if task == 'cross_validate':
        cv_folds_file = config.get("Input", "cv_folds_file")
        if cv_folds_file:
            config.set("Input", "cv_folds_file",
                       join(train_dir, cv_folds_file))
    if task == 'predict' or task == 'evaluate':
        config.set("Input", "test_directory", test_dir)
        custom_learner_path = config.get("Input", "custom_learner_path")
    custom_learner_abs_path = join(_my_dir, custom_learner_path)
    config.set("Input", "custom_learner_path", custom_learner_abs_path)
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}.cfg'.format(config_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def fill_in_config_paths_for_single_file(config_template_path, train_file,
                                         test_file, train_directory='',
                                         test_directory=''):
    
    train_dir = join(_my_dir, 'train')
    test_dir = join(_my_dir, 'test')
    output_dir = join(_my_dir, 'output')
    config = _setup_config_parser(config_template_path, validate=False)
    task = config.get("General", "task")
    config.set("Input", "train_file", join(train_dir, train_file))
    if task == 'predict' or task == 'evaluate':
        config.set("Input", "test_file", join(test_dir, test_file))
    if train_directory:
        config.set("Input", "train_directory", join(train_dir, train_directory))
    if test_directory:
        config.set("Input", "test_directory", join(test_dir, test_directory))
    to_fill_in = ['log', 'predictions']
    if task != 'cross_validate':
        to_fill_in.append('models')
    if task == 'evaluate' or task == 'cross_validate':
        to_fill_in.append('results')
    for d in to_fill_in:
        config.set("Output", d, join(output_dir))
    if task == 'cross_validate':
        cv_folds_file = config.get("Input", "cv_folds_file")
        if cv_folds_file:
            config.set("Input", "cv_folds_file",
                       join(train_dir, cv_folds_file))
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}.cfg'.format(config_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def fill_in_config_options(config_template_path,
                           values_to_fill_dict,
                           sub_prefix):
    
    config = _setup_config_parser(config_template_path, validate=False)
                            to_fill_in = {'General': ['experiment_name', 'task'],
                  'Input': ['train_directory', 'train_file', 'test_directory',
                            'test_file', 'featuresets', 'featureset_names',
                            'feature_hasher', 'hasher_features', 'learners',
                            'sampler', 'shuffle', 'feature_scaling',
                            'learning_curve_cv_folds_list',
                            'learning_curve_train_sizes', 'fixed_parameters',
                            'num_cv_folds', 'bad_option', 'duplicate_option'],
                  'Tuning': ['probability', 'grid_search', 'objective',
                             'param_grids', 'objectives', 'duplicate_option'],
                  'Output': ['results', 'log', 'models',
                             'predictions']}
    for section in to_fill_in:
        for param_name in to_fill_in[section]:
            if param_name in values_to_fill_dict:
                config.set(section, param_name,
                           values_to_fill_dict[param_name])
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}_{}.cfg'.format(config_prefix, sub_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def fill_in_config_paths_for_fancy_output(config_template_path):
    
    train_dir = join(_my_dir, 'train')
    test_dir = join(_my_dir, 'test')
    output_dir = join(_my_dir, 'output')
    config = _setup_config_parser(config_template_path, validate=False)
    config.set("Input", "train_file", join(train_dir, "fancy_train.jsonlines"))
    config.set("Input", "test_file", join(test_dir,
                                          "fancy_test.jsonlines"))
    config.set("Output", "results", output_dir)
    config.set("Output", "log", output_dir)
    config.set("Output", "predictions", output_dir)
    config_prefix = re.search(r'^(.*)\.template\.cfg',
                              config_template_path).groups()[0]
    new_config_path = '{}.cfg'.format(config_prefix)
    with open(new_config_path, 'w') as new_config_file:
        config.write(new_config_file)
    return new_config_path

def make_classification_data(num_examples=100, train_test_ratio=0.5,
                             num_features=10, use_feature_hashing=False,
                             feature_bins=4, num_labels=2,
                             empty_labels=False, feature_prefix='f',
                             class_weights=None, non_negative=False,
                             one_string_feature=False, num_string_values=4,
                             random_state=1234567890):
        num_numeric_features = (num_features - 1 if one_string_feature else
                            num_features)
    X, y = make_classification(n_samples=num_examples,
                               n_features=num_numeric_features,
                               n_informative=num_numeric_features,
                               n_redundant=0, n_classes=num_labels,
                               weights=class_weights,
                               random_state=random_state)
            if non_negative:
        X = abs(X)
            ids = ['EXAMPLE_{}'.format(n) for n in range(1, num_examples + 1)]
            if one_string_feature:
        prng = RandomState(random_state)
        random_indices = prng.random_integers(0, num_string_values - 1,
                                              num_examples)
        possible_values = [chr(x) for x in range(97, 97 + num_string_values)]
        string_feature_values = [possible_values[i] for i in random_indices]
        string_feature_column = np.array(string_feature_values,
                                         dtype=object).reshape(100, 1)
        X = np.append(X, string_feature_column, 1)
        feature_names = ['{}{:02d}'.format(feature_prefix, n) for n in
                     range(1, num_features + 1)]
    features = [dict(zip(feature_names, row)) for row in X]
        num_train_examples = int(round(train_test_ratio * num_examples))
    train_features, test_features = (features[:num_train_examples],
                                     features[num_train_examples:])
    train_y, test_y = y[:num_train_examples], y[num_train_examples:]
    train_ids, test_ids = ids[:num_train_examples], ids[num_train_examples:]
        train_labels = None if empty_labels else train_y
    test_labels = None if empty_labels else test_y
            vectorizer = (FeatureHasher(n_features=feature_bins)
                  if use_feature_hashing else None)
    train_fs = FeatureSet('classification_train', train_ids,
                          labels=train_labels, features=train_features,
                          vectorizer=vectorizer)
    if train_test_ratio < 1.0:
        test_fs = FeatureSet('classification_test', test_ids,
                             labels=test_labels, features=test_features,
                             vectorizer=vectorizer)
    else:
        test_fs = None
    return (train_fs, test_fs)

def make_regression_data(num_examples=100, train_test_ratio=0.5,
                         num_features=2, sd_noise=1.0,
                         use_feature_hashing=False,
                         feature_bins=4,
                         start_feature_num=1,
                         random_state=1234567890):
        X, y, weights = make_regression(n_samples=num_examples,
                                    n_features=num_features,
                                    noise=sd_noise, random_state=random_state,
                                    coef=True)
            ids = ['EXAMPLE_{}'.format(n) for n in range(1, num_examples + 1)]
        feature_names = ['f{:02d}'.format(n) for n
                     in range(start_feature_num,
                              start_feature_num + num_features)]
    features = [dict(zip(feature_names, row)) for row in X]
        weightdict = dict(zip(feature_names, weights))
        num_train_examples = int(round(train_test_ratio * num_examples))
    train_features, test_features = (features[:num_train_examples],
                                     features[num_train_examples:])
    train_y, test_y = y[:num_train_examples], y[num_train_examples:]
    train_ids, test_ids = ids[:num_train_examples], ids[num_train_examples:]
            vectorizer = (FeatureHasher(n_features=feature_bins) if
                  use_feature_hashing else None)
    train_fs = FeatureSet('regression_train', train_ids,
                          labels=train_y, features=train_features,
                          vectorizer=vectorizer)
    test_fs = FeatureSet('regression_test', test_ids,
                         labels=test_y, features=test_features,
                         vectorizer=vectorizer)
    return (train_fs, test_fs, weightdict)

def make_sparse_data(use_feature_hashing=False):
            X, y = make_classification(n_samples=500, n_features=3,
                               n_informative=3, n_redundant=0,
                               n_classes=2, random_state=1234567890)
            X = np.abs(X)
        X[np.where(X == 0)] += 1
            ids = ['EXAMPLE_{}'.format(n) for n in range(1, 501)]
            feature_names = ['f{}'.format(n) for n in range(1, 6)]
    features = []
    for row in X:
        row = [0] + row.tolist() + [0]
        features.append(dict(zip(feature_names, row)))
        vectorizer = FeatureHasher(n_features=4) if use_feature_hashing else None
    train_fs = FeatureSet('train_sparse', ids,
                          features=features, labels=y,
                          vectorizer=vectorizer)
        X, y = make_classification(n_samples=100, n_features=4,
                               n_informative=4, n_redundant=0,
                               n_classes=2, random_state=1234567890)
    X = np.abs(X)
    X[np.where(X == 0)] += 1
    ids = ['EXAMPLE_{}'.format(n) for n in range(1, 101)]
            feature_names = ['f{}'.format(n) for n in range(1, 6)]
    features = []
    for row in X:
        row = row.tolist()
        row = row[:3] + [0] + row[3:]
        features.append(dict(zip(feature_names, row)))
    test_fs = FeatureSet('test_sparse', ids,
                         features=features, labels=y,
                         vectorizer=vectorizer)
    return train_fs, test_fs
from sklearn.linear_model import LogisticRegression

class CustomLogisticRegressionWrapper(LogisticRegression):
    pass
from collections import Counter
import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin

class MajorityClassLearner(BaseEstimator, ClassifierMixin):
    def __init__(self):
        self.majority_class = None
    def fit(self, X, y):
        counts = Counter(y)
        max_count = -1
        for label, count in counts.items():
            if count > max_count:
                self.majority_class = label
                max_count = count
    def predict(self, X):
        return np.array([self.majority_class for x in range(X.shape[0])])
from setuptools import setup, find_packages, Extension
from codecs import open
from os import path
try:
    import numpy as np
except ImportError:
    exit('Please install numpy>=1.11.2 first.')
try:
    from Cython.Build import cythonize
    from Cython.Distutils import build_ext
except ImportError:
    USE_CYTHON = False
else:
    USE_CYTHON = True
__version__ = '1.0.2'
here = path.abspath(path.dirname(__file__))
try:
    import pypandoc
    long_description = pypandoc.convert(path.join(here, 'README.md'), 'rst')
except(IOError, ImportError):
    with open(path.join(here, 'README.md'), encoding='utf-8') as f:
        long_description = f.read()
with open(path.join(here, 'requirements.txt'), encoding='utf-8') as f:
    all_reqs = f.read().split('\n')
install_requires = [x.strip() for x in all_reqs if 'git+' not in x]
dependency_links = [x.strip().replace('git+', '') for x in all_reqs if x.startswith('git+')]
cmdclass = {}
ext = '.pyx' if USE_CYTHON else '.c'
extensions = [Extension('surprise.similarities',
                       ['surprise/similarities' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.matrix_factorization',
                        ['surprise/prediction_algorithms/matrix_factorization' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.optimize_baselines',
                        ['surprise/prediction_algorithms/optimize_baselines' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.slope_one',
                        ['surprise/prediction_algorithms/slope_one' + ext],
                        include_dirs=[np.get_include()]),
              Extension('surprise.prediction_algorithms.co_clustering',
                        ['surprise/prediction_algorithms/co_clustering' + ext],
                        include_dirs=[np.get_include()]),
             ]
if USE_CYTHON:
    ext_modules = cythonize(extensions)
    cmdclass.update({'build_ext': build_ext})
else:
	ext_modules = extensions
setup(
    name='scikit-surprise',
    author='Nicolas Hug',
    author_email='contact@nicolas-hug.com',
    description=('An easy-to-use library for recommender systems.'),
    long_description=long_description,
    version=__version__,
    url='http://surpriselib.com',
    license='GPLv3+',
    classifiers=[
      'Development Status :: 5 - Production/Stable',
      'Intended Audience :: Developers',
      'Intended Audience :: Education',
      'Intended Audience :: Science/Research',
      'Topic :: Scientific/Engineering',
      'License :: OSI Approved :: BSD License',
      'Programming Language :: Python :: 3',
      'Programming Language :: Python :: 2.7',
    ],
    keywords='recommender recommendation system',
    packages=find_packages(exclude=['tests*']),
    include_package_data=True,
    ext_modules = ext_modules,
    cmdclass=cmdclass,
    install_requires=install_requires,
    dependency_links=dependency_links,
    entry_points={'console_scripts':
                 ['surprise = surprise.__main__:main']},
)
import sys
import os
import shlex


extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.napoleon',
    'sphinx.ext.coverage',
    'sphinx.ext.mathjax',
    'sphinx.ext.viewcode',
    'sphinx.ext.graphviz',
    'sphinx.ext.inheritance_diagram',
    'sphinx.ext.autosummary',
    'sphinxcontrib.bibtex',
    'sphinxcontrib.spelling',
]
templates_path = ['.templates']
source_suffix = '.rst'

master_doc = 'index'
project = 'Surprise'
copyright = '2015, Nicolas Hug'
author = 'Nicolas Hug'
version = '0'
release = '1'
language = None

exclude_patterns = []

add_function_parentheses = True

pygments_style = 'sphinx'

todo_include_todos = False

import sphinx_rtd_theme
html_theme = "sphinx_rtd_theme"
html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]












htmlhelp_basename = 'Surprisedoc'

latex_elements = {
'pointsize': '12pt',

}
latex_documents = [
  (master_doc, 'Surprise.tex', 'Surprise Documentation',
   'Nicolas Hug', 'manual'),
]




man_pages = [
    (master_doc, 'surprise', 'Surprise Documentation',
     [author], 1)
]


texinfo_documents = [
  (master_doc, 'Surprise', 'Surprise Documentation',
   author, 'Surprise', 'One line description of project.',
   'Miscellaneous'),
]


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import BaselineOnly
from surprise import KNNBasic
from surprise import Dataset
from surprise import evaluate

data = Dataset.load_builtin('ml-100k')
print('Using ALS')
bsl_options = {'method': 'als',
               'n_epochs': 5,
               'reg_u': 12,
               'reg_i': 5
               }
algo = BaselineOnly(bsl_options=bsl_options)
evaluate(algo, data)
print('Using SGD')
bsl_options = {'method': 'sgd',
               'learning_rate': .00005,
               }
algo = BaselineOnly(bsl_options=bsl_options)
evaluate(algo, data)
print('Using ALS with pearson_baseline similarity')
bsl_options = {'method': 'als',
               'n_epochs': 20,
               }
sim_options = {'name': 'pearson_baseline'}
algo = KNNBasic(bsl_options=bsl_options, sim_options=sim_options)
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import SVD
from surprise import Dataset
from surprise import evaluate, print_perf

data = Dataset.load_builtin('ml-100k')
data.split(n_folds=3)
algo = SVD()
perf = evaluate(algo, data, measures=['RMSE', 'MAE'])
print_perf(perf)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import GridSearch
from surprise import SVD
from surprise import Dataset
param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],
              'reg_all': [0.4, 0.6]}
grid_search = GridSearch(SVD, param_grid, measures=['RMSE', 'FCP'])
data = Dataset.load_builtin('ml-100k')
data.split(n_folds=3)
grid_search.evaluate(data)
print(grid_search.best_score['RMSE'])
print(grid_search.best_params['RMSE'])
print(grid_search.best_score['FCP'])
print(grid_search.best_params['FCP'])
import pandas as pd  
results_df = pd.DataFrame.from_dict(grid_search.cv_results)
print(results_df)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import BaselineOnly
from surprise import Dataset
from surprise import accuracy
data = Dataset.load_builtin('ml-100k')
data.split(n_folds=3)
algo = BaselineOnly()
for trainset, testset in data.folds():
        algo.train(trainset)
    predictions = algo.test(testset)
        rmse = accuracy.rmse(predictions, verbose=True)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import os
from surprise import BaselineOnly
from surprise import Dataset
from surprise import evaluate
from surprise import Reader
file_path = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/u.data')
reader = Reader(line_format='user item rating timestamp', sep='\t')
data = Dataset.load_from_file(file_path, reader=reader)
data.split(n_folds=5)
algo = BaselineOnly()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import os
from surprise import BaselineOnly
from surprise import Dataset
from surprise import evaluate
from surprise import Reader
files_dir = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/')
reader = Reader('ml-100k')
train_file = files_dir + 'u%d.base'
test_file = files_dir + 'u%d.test'
folds_files = [(train_file % i, test_file % i) for i in (1, 2, 3, 4, 5)]
data = Dataset.load_from_folds(folds_files, reader=reader)
algo = BaselineOnly()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import KNNBasic
from surprise import Dataset
from surprise import evaluate
data = Dataset.load_builtin('ml-100k')
trainset = data.build_full_trainset()
algo = KNNBasic()
algo.train(trainset)

uid = str(196)  iid = str(302)  
pred = algo.predict(uid, iid, r_ui=4, verbose=True)

data.split(n_folds=3)
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import KNNBasic
from surprise import Dataset
from surprise import evaluate

data = Dataset.load_builtin('ml-100k')
sim_options = {'name': 'cosine',
               'user_based': False                 }
algo = KNNBasic(sim_options=sim_options)
evaluate(algo, data)
sim_options = {'name': 'pearson_baseline',
               'shrinkage': 0                 }
algo = KNNBasic(sim_options=sim_options)
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate

class MyOwnAlgorithm(AlgoBase):
    def __init__(self):
                AlgoBase.__init__(self)
    def estimate(self, u, i):
        sum_means = self.trainset.global_mean
        div = 1
        if self.trainset.knows_user(u):
            sum_means += np.mean([r for (_, r) in self.trainset.ur[u]])
            div += 1
        if self.trainset.knows_item(i):
            sum_means += np.mean([r for (_, r) in self.trainset.ir[i]])
            div += 1
        return sum_means / div

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate

class MyOwnAlgorithm(AlgoBase):
    def __init__(self):
                AlgoBase.__init__(self)
    def estimate(self, u, i):
        return 3

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate

class MyOwnAlgorithm(AlgoBase):
    def __init__(self):
                AlgoBase.__init__(self)
    def train(self, trainset):
                AlgoBase.train(self, trainset)
                        self.the_mean = np.mean([r for (_, _, r) in
                                 self.trainset.all_ratings()])
    def estimate(self, u, i):
        return self.the_mean

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from surprise import AlgoBase
from surprise import Dataset
from surprise import evaluate
from surprise import PredictionImpossible

class MyOwnAlgorithm(AlgoBase):
    def __init__(self, sim_options={}, bsl_options={}):
        AlgoBase.__init__(self, sim_options=sim_options,
                          bsl_options=bsl_options)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
                self.bu, self.bi = self.compute_baselines()
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
                        neighbors = [(v, self.sim[u, v]) for (v, r) in self.trainset.ir[i]]
                neighbors = sorted(neighbors, key=lambda x: x[1], reverse=True)
        print('The 3 nearest neighbors of user', str(u), 'are:')
        for v, sim_uv in neighbors[:3]:
            print('user {0:} with sim {1:1.2f}'.format(v, sim_uv))
                bsl = self.trainset.global_mean + self.bu[u] + self.bi[i]
        return bsl

data = Dataset.load_builtin('ml-100k')
algo = MyOwnAlgorithm()
evaluate(algo, data)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import defaultdict
import numpy as np
from six import iteritems

def rmse(predictions, verbose=True):
    
    if not predictions:
        raise ValueError('Prediction list is empty.')
    mse = np.mean([float((true_r - est)**2)
                   for (_, _, true_r, est, _) in predictions])
    rmse_ = np.sqrt(mse)
    if verbose:
        print('RMSE: {0:1.4f}'.format(rmse_))
    return rmse_

def mae(predictions, verbose=True):
    
    if not predictions:
        raise ValueError('Prediction list is empty.')
    mae_ = np.mean([float(abs(true_r - est))
                    for (_, _, true_r, est, _) in predictions])
    if verbose:
        print('MAE:  {0:1.4f}'.format(mae_))
    return mae_

def fcp(predictions, verbose=True):
    
    if not predictions:
        raise ValueError('Prediction list is empty.')
    predictions_u = defaultdict(list)
    nc_u = defaultdict(int)
    nd_u = defaultdict(int)
    for u0, _, r0, est, _ in predictions:
        predictions_u[u0].append((r0, est))
    for u0, preds in iteritems(predictions_u):
        for r0i, esti in preds:
            for r0j, estj in preds:
                if esti > estj and r0i > r0j:
                    nc_u[u0] += 1
                if esti >= estj and r0i < r0j:
                    nd_u[u0] += 1
    nc = np.mean(list(nc_u.values())) if nc_u else 0
    nd = np.mean(list(nd_u.values())) if nd_u else 0
    try:
        fcp = nc / (nc + nd)
    except ZeroDivisionError:
        raise ValueError('cannot compute fcp on this list of prediction. ' +
                         'Does every user have at least two predictions?')
    if verbose:
        print('FCP:  {0:1.4f}'.format(fcp))
    return fcp

from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import defaultdict
from collections import namedtuple
import sys
import os
import zipfile
import itertools
import random
import numpy as np
from six.moves import input
from six.moves.urllib.request import urlretrieve
from six.moves import range
from six import iteritems

DATASETS_DIR = os.path.expanduser('~') + '/.surprise_data/'
BuiltinDataset = namedtuple('BuiltinDataset', ['url', 'path', 'reader_params'])
BUILTIN_DATASETS = {
    'ml-100k':
        BuiltinDataset(
            url='http://files.grouplens.org/datasets/movielens/ml-100k.zip',
            path=DATASETS_DIR + 'ml-100k/ml-100k/u.data',
            reader_params=dict(line_format='user item rating timestamp',
                               rating_scale=(1, 5),
                               sep='\t')
        ),
    'ml-1m':
        BuiltinDataset(
            url='http://files.grouplens.org/datasets/movielens/ml-1m.zip',
            path=DATASETS_DIR + 'ml-1m/ml-1m/ratings.dat',
            reader_params=dict(line_format='user item rating timestamp',
                               rating_scale=(1, 5),
                               sep='::')
        ),
    'jester':
        BuiltinDataset(
            url='http://eigentaste.berkeley.edu/dataset/jester_dataset_2.zip',
            path=DATASETS_DIR + 'jester/jester_ratings.dat',
            reader_params=dict(line_format='user item rating',
                               rating_scale=(-10, 10))
        )
}

class Dataset:
    
    def __init__(self, reader):
        self.reader = reader
    @classmethod
    def load_builtin(cls, name='ml-100k'):
        
        try:
            dataset = BUILTIN_DATASETS[name]
        except KeyError:
            raise ValueError('unknown dataset ' + name +
                             '. Accepted values are ' +
                             ', '.join(BUILTIN_DATASETS.keys()) + '.')
                if not os.path.isfile(dataset.path):
            answered = False
            while not answered:
                print('Dataset ' + name + ' could not be found. Do you want '
                      'to download it? [Y/n] ', end='')
                choice = input().lower()
                if choice in ['yes', 'y', '', 'omg this is so nice of you!!']:
                    answered = True
                elif choice in ['no', 'n', 'hell no why would i want that?!']:
                    answered = True
                    print("Ok then, I'm out!")
                    sys.exit()
            if not os.path.exists(DATASETS_DIR):
                os.makedirs(DATASETS_DIR)
            print('Trying to download dataset from ' + dataset.url + '...')
            urlretrieve(dataset.url, DATASETS_DIR + 'tmp.zip')
            with zipfile.ZipFile(DATASETS_DIR + 'tmp.zip', 'r') as tmp_zip:
                tmp_zip.extractall(DATASETS_DIR + name)
            os.remove(DATASETS_DIR + 'tmp.zip')
            print('Done! Dataset', name, 'has been saved to', DATASETS_DIR +
                  name)
        reader = Reader(**dataset.reader_params)
        return cls.load_from_file(file_path=dataset.path, reader=reader)
    @classmethod
    def load_from_file(cls, file_path, reader):
        
        return DatasetAutoFolds(ratings_file=file_path, reader=reader)
    @classmethod
    def load_from_folds(cls, folds_files, reader):
        
        return DatasetUserFolds(folds_files=folds_files, reader=reader)
    def read_ratings(self, file_name):
        
        with open(os.path.expanduser(file_name)) as f:
            raw_ratings = [self.reader.parse_line(line) for line in
                           itertools.islice(f, self.reader.skip_lines, None)]
        return raw_ratings
    def folds(self):
        
        for raw_trainset, raw_testset in self.raw_folds():
            trainset = self.construct_trainset(raw_trainset)
            testset = self.construct_testset(raw_testset)
            yield trainset, testset
    def construct_trainset(self, raw_trainset):
        raw2inner_id_users = {}
        raw2inner_id_items = {}
        current_u_index = 0
        current_i_index = 0
        ur = defaultdict(list)
        ir = defaultdict(list)
                for urid, irid, r, timestamp in raw_trainset:
            try:
                uid = raw2inner_id_users[urid]
            except KeyError:
                uid = current_u_index
                raw2inner_id_users[urid] = current_u_index
                current_u_index += 1
            try:
                iid = raw2inner_id_items[irid]
            except KeyError:
                iid = current_i_index
                raw2inner_id_items[irid] = current_i_index
                current_i_index += 1
            ur[uid].append((iid, r))
            ir[iid].append((uid, r))
        n_users = len(ur)          n_items = len(ir)          n_ratings = len(raw_trainset)
        trainset = Trainset(ur,
                            ir,
                            n_users,
                            n_items,
                            n_ratings,
                            self.reader.rating_scale,
                            self.reader.offset,
                            raw2inner_id_users,
                            raw2inner_id_items)
        return trainset
    def construct_testset(self, raw_testset):
        return [(ruid, riid, r_ui_trans)
                for (ruid, riid, r_ui_trans, _) in raw_testset]

class DatasetUserFolds(Dataset):
    
    def __init__(self, folds_files=None, reader=None):
        Dataset.__init__(self, reader)
        self.folds_files = folds_files
                for train_test_files in self.folds_files:
            for f in train_test_files:
                if not os.path.isfile(os.path.expanduser(f)):
                    raise ValueError('File ' + str(f) + ' does not exist.')
    def raw_folds(self):
        for train_file, test_file in self.folds_files:
            raw_train_ratings = self.read_ratings(train_file)
            raw_test_ratings = self.read_ratings(test_file)
            yield raw_train_ratings, raw_test_ratings

class DatasetAutoFolds(Dataset):
    
    def __init__(self, ratings_file=None, reader=None):
        Dataset.__init__(self, reader)
        self.ratings_file = ratings_file
        self.n_folds = 5
        self.shuffle = True
        self.raw_ratings = self.read_ratings(self.ratings_file)
    def build_full_trainset(self):
        
        return self.construct_trainset(self.raw_ratings)
    def raw_folds(self):
        if self.shuffle:
            random.shuffle(self.raw_ratings)
            self.shuffle = False  
        def k_folds(seq, n_folds):
            
            if n_folds > len(seq) or n_folds < 2:
                raise ValueError('Incorrect value for n_folds.')
            start, stop = 0, 0
            for fold_i in range(n_folds):
                start = stop
                stop += len(seq) // n_folds
                if fold_i < len(seq) % n_folds:
                    stop += 1
                yield seq[:start] + seq[stop:], seq[start:stop]
        return k_folds(self.raw_ratings, self.n_folds)
    def split(self, n_folds=5, shuffle=True):
        
        self.n_folds = n_folds
        self.shuffle = shuffle

class Reader():
    
    def __init__(self, name=None, line_format=None, sep=None,
                 rating_scale=(1, 5), skip_lines=0):
        if name:
            try:
                self.__init__(**BUILTIN_DATASETS[name].reader_params)
            except KeyError:
                raise ValueError('unknown reader ' + name +
                                 '. Accepted values are ' +
                                 ', '.join(BUILTIN_DATASETS.keys()) + '.')
        else:
            self.sep = sep
            self.skip_lines = skip_lines
            self.rating_scale = rating_scale
            lower_bound, higher_bound = rating_scale
            self.offset = -lower_bound + 1 if lower_bound <= 0 else 0
            splitted_format = line_format.split()
            entities = ['user', 'item', 'rating']
            if 'timestamp' in splitted_format:
                self.with_timestamp = True
                entities.append('timestamp')
            else:
                self.with_timestamp = False
                        if any(field not in entities for field in splitted_format):
                raise ValueError('line_format parameter is incorrect.')
            self.indexes = [splitted_format.index(entity) for entity in
                            entities]
    def parse_line(self, line):
        '''Parse a line.
        Ratings are translated so that they are all strictly positive.
        Args:
            line(str): The line to parse
        Returns:
            tuple: User id, item id, rating and timestamp. The timestamp is set
            to ``None`` if it does no exist.
            '''
        line = line.split(self.sep)
        try:
            if self.with_timestamp:
                uid, iid, r, timestamp = (line[i].strip()
                                          for i in self.indexes)
            else:
                uid, iid, r = (line[i].strip()
                               for i in self.indexes)
                timestamp = None
        except IndexError:
            raise ValueError(('Impossible to parse line.' +
                              ' Check the line_format  and sep parameters.'))
        return uid, iid, float(r) + self.offset, timestamp

class Trainset:
    
    def __init__(self, ur, ir, n_users, n_items, n_ratings, rating_scale,
                 offset, raw2inner_id_users, raw2inner_id_items):
        self.ur = ur
        self.ir = ir
        self.n_users = n_users
        self.n_items = n_items
        self.n_ratings = n_ratings
        self.rating_scale = rating_scale
        self.offset = offset
        self._raw2inner_id_users = raw2inner_id_users
        self._raw2inner_id_items = raw2inner_id_items
        self._global_mean = None
    def knows_user(self, uid):
        
        return uid in self.ur
    def knows_item(self, iid):
        
        return iid in self.ir
    def to_inner_uid(self, ruid):
        
        try:
            return self._raw2inner_id_users[ruid]
        except KeyError:
            raise ValueError(('User ' + str(ruid) +
                              ' is not part of the trainset.'))
    def to_inner_iid(self, riid):
        
        try:
            return self._raw2inner_id_items[riid]
        except KeyError:
            raise ValueError(('Item ' + str(riid) +
                              ' is not part of the trainset.'))
    def all_ratings(self):
        
        for u, u_ratings in iteritems(self.ur):
            for i, r in u_ratings:
                yield u, i, r
    def all_users(self):
                return range(self.n_users)
    def all_items(self):
                return range(self.n_items)
    @property
    def global_mean(self):
                if self._global_mean is None:
            self._global_mean = np.mean([r for (_, _, r) in
                                         self.all_ratings()])
        return self._global_mean
import pickle

def dump(file_name, predictions, trainset=None, algo=None):
    
    dump_obj = dict()
    dump_obj['predictions'] = predictions
    if trainset is not None:
        dump_obj['trainset'] = trainset
    if algo is not None:
        dump_obj['algo'] = algo.__dict__          dump_obj['algo']['name'] = algo.__class__.__name__
    pickle.dump(dump_obj, open(file_name, 'wb'))
    print('The dump has been saved as file', file_name)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import defaultdict
import time
import os
import numpy as np
from six import iteritems
from six import itervalues
from itertools import product
from . import accuracy
from .dump import dump

def evaluate(algo, data, measures=['rmse', 'mae'], with_dump=False,
             dump_dir=None, verbose=1):
    
    performances = CaseInsensitiveDefaultDict(list)
    print('Evaluating {0} of algorithm {1}.'.format(
          ', '.join((m.upper() for m in measures)),
          algo.__class__.__name__))
    print()
    for fold_i, (trainset, testset) in enumerate(data.folds()):
        if verbose:
            print('-' * 12)
            print('Fold ' + str(fold_i + 1))
                algo.train(trainset)
        predictions = algo.test(testset, verbose=(verbose == 2))
                for measure in measures:
            f = getattr(accuracy, measure.lower())
            performances[measure].append(f(predictions, verbose=verbose))
        if with_dump:
            if dump_dir is None:
                dump_dir = os.path.expanduser('~') + '/.surprise_data/dumps/'
            if not os.path.exists(dump_dir):
                os.makedirs(dump_dir)
            date = time.strftime('%y%m%d-%Hh%Mm%S', time.localtime())
            file_name = date + '-' + algo.__class__.__name__
            file_name += '-fold{0}'.format(fold_i + 1)
            file_name = os.path.join(dump_dir, file_name)
            dump(file_name, predictions, trainset, algo)
    if verbose:
        print('-' * 12)
        print('-' * 12)
        for measure in measures:
            print('Mean {0:4s}: {1:1.4f}'.format(
                  measure.upper(), np.mean(performances[measure])))
        print('-' * 12)
        print('-' * 12)
    return performances

class GridSearch:
    
    def __init__(self, algo_class, param_grid, measures=['rmse', 'mae'],
                 verbose=1):
        self.best_params = CaseInsensitiveDefaultDict(list)
        self.best_index = CaseInsensitiveDefaultDict(list)
        self.best_score = CaseInsensitiveDefaultDict(list)
        self.best_estimator = CaseInsensitiveDefaultDict(list)
        self.cv_results = defaultdict(list)
        self.algo_class = algo_class
        self.param_grid = param_grid
        self.measures = [measure.upper() for measure in measures]
        self.verbose = verbose
        self.param_combinations = [dict(zip(param_grid, v)) for v in
                                   product(*param_grid.values())]
    def evaluate(self, data):
        
        num_of_combinations = len(self.param_combinations)
        params = []
        scores = []
                for combination_index, combination in enumerate(
                self.param_combinations):
            params.append(combination)
            if self.verbose >= 1:
                print('-' * 12)
                print('Parameters combination {} of {}'.
                      format(combination_index + 1, num_of_combinations))
                print('params: ', combination)
                        algo_instance = self.algo_class(**combination)
            evaluate_results = evaluate(algo_instance, data,
                                        measures=self.measures,
                                        verbose=(self.verbose == 2))
                        mean_score = {}
            for measure in self.measures:
                mean_score[measure] = np.mean(evaluate_results[measure])
            scores.append(mean_score)
            if self.verbose == 1:
                print('-' * 12)
                for measure in self.measures:
                    print('Mean {0:4s}: {1:1.4f}'.format(
                        measure, mean_score[measure]))
                print('-' * 12)
                self.cv_results['params'] = params
        self.cv_results['scores'] = scores
                for param, score in zip(params, scores):
            for param_key, score_key in zip(param.keys(), score.keys()):
                self.cv_results[param_key].append(param[param_key])
                self.cv_results[score_key].append(score[score_key])
                for measure in self.measures:
            if measure == 'FCP':
                best_dict = max(self.cv_results['scores'],
                                key=lambda x: x[measure])
            else:
                best_dict = min(self.cv_results['scores'],
                                key=lambda x: x[measure])
            self.best_score[measure] = best_dict[measure]
            self.best_index[measure] = self.cv_results['scores'].index(
                best_dict)
            self.best_params[measure] = self.cv_results['params'][
                self.best_index[measure]]
            self.best_estimator[measure] = self.algo_class(
                **self.best_params[measure])

class CaseInsensitiveDefaultDict(defaultdict):
        def __setitem__(self, key, value):
        super(CaseInsensitiveDefaultDict, self).__setitem__(key.lower(), value)
    def __getitem__(self, key):
        return super(CaseInsensitiveDefaultDict, self).__getitem__(key.lower())

def print_perf(performances):
        n_folds = [len(values) for values in itervalues(performances)][0]
    row_format = '{:<8}' * (n_folds + 2)
    s = row_format.format(
        '',
        *['Fold {0}'.format(i + 1) for i in range(n_folds)] + ['Mean'])
    s += '\n'
    s += '\n'.join(row_format.format(
        key.upper(),
        *['{:1.4f}'.format(v) for v in vals] +
        ['{:1.4f}'.format(np.mean(vals))])
        for (key, vals) in iteritems(performances))
    print(s)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from six import iteritems

def cosine(n_x, yr, min_support):
    
        cdef np.ndarray[np.int_t, ndim=2] prods
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.int_t, ndim=2] sqi
        cdef np.ndarray[np.int_t, ndim=2] sqj
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef int xi, xj, ri, rj
    cdef int min_sprt = min_support
    prods = np.zeros((n_x, n_x), np.int)
    freq = np.zeros((n_x, n_x), np.int)
    sqi = np.zeros((n_x, n_x), np.int)
    sqj = np.zeros((n_x, n_x), np.int)
    sim = np.zeros((n_x, n_x), np.double)
    for y, y_ratings in iteritems(yr):
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                freq[xi, xj] += 1
                prods[xi, xj] += ri * rj
                sqi[xi, xj] += ri**2
                sqj[xi, xj] += rj**2
    for xi in range(n_x):
        sim[xi, xi] = 1
        for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] = 0
            else:
                denum = np.sqrt(sqi[xi, xj] * sqj[xi, xj])
                sim[xi, xj] = prods[xi, xj] / denum
            sim[xj, xi] = sim[xi, xj]
    return sim

def msd(n_x, yr, min_support):
    
        cdef np.ndarray[np.double_t, ndim=2] sq_diff
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef int xi, xj, ri, rj
    cdef int min_sprt = min_support
    sq_diff = np.zeros((n_x, n_x), np.double)
    freq = np.zeros((n_x, n_x), np.int)
    sim = np.zeros((n_x, n_x), np.double)
    for y, y_ratings in iteritems(yr):
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                sq_diff[xi, xj] += (ri - rj)**2
                freq[xi, xj] += 1
    for xi in range(n_x):
        sim[xi, xi] = 1          for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] == 0
            else:
                                sim[xi, xj] = 1 / (sq_diff[xi, xj] / freq[xi, xj] + 1)
            sim[xj, xi] = sim[xi, xj]
    return sim

def pearson(n_x, yr, min_support):
    
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.int_t, ndim=2] prods
        cdef np.ndarray[np.int_t, ndim=2] sqi
        cdef np.ndarray[np.int_t, ndim=2] sqj
        cdef np.ndarray[np.int_t, ndim=2] si
        cdef np.ndarray[np.int_t, ndim=2] sj
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef int xi, xj, ri, rj
    cdef int min_sprt = min_support
    freq = np.zeros((n_x, n_x), np.int)
    prods = np.zeros((n_x, n_x), np.int)
    sqi = np.zeros((n_x, n_x), np.int)
    sqj = np.zeros((n_x, n_x), np.int)
    si = np.zeros((n_x, n_x), np.int)
    sj = np.zeros((n_x, n_x), np.int)
    sim = np.zeros((n_x, n_x), np.double)
    for y, y_ratings in iteritems(yr):
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                prods[xi, xj] += ri * rj
                freq[xi, xj] += 1
                sqi[xi, xj] += ri**2
                sqj[xi, xj] += rj**2
                si[xi, xj] += ri
                sj[xi, xj] += rj
    for xi in range(n_x):
        sim[xi, xi] = 1
        for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] == 0
            else:
                n = freq[xi, xj]
                num = n * prods[xi, xj] - si[xi, xj] * sj[xi, xj]
                denum = np.sqrt((n * sqi[xi, xj] - si[xi, xj]**2) *
                                (n * sqj[xi, xj] - sj[xi, xj]**2))
                if denum == 0:
                    sim[xi, xj] = 0
                else:
                    sim[xi, xj] = num / denum
            sim[xj, xi] = sim[xi, xj]
    return sim

def pearson_baseline(n_x, yr, min_support, global_mean, x_biases, y_biases,
                     shrinkage=100):
    
        cdef np.ndarray[np.int_t, ndim=2] freq
        cdef np.ndarray[np.double_t, ndim=2] prods
        cdef np.ndarray[np.double_t, ndim=2] sq_diff_i
        cdef np.ndarray[np.double_t, ndim=2] sq_diff_j
        cdef np.ndarray[np.double_t, ndim=2] sim
    cdef np.ndarray[np.double_t, ndim=1] x_biases_
    cdef np.ndarray[np.double_t, ndim=1] y_biases_
    cdef int xi, xj
    cdef double ri, rj, diff_i, diff_j, partial_bias
    cdef int min_sprt = min_support
    cdef double global_mean_ = global_mean
    freq = np.zeros((n_x, n_x), np.int)
    prods = np.zeros((n_x, n_x), np.double)
    sq_diff_i = np.zeros((n_x, n_x), np.double)
    sq_diff_j = np.zeros((n_x, n_x), np.double)
    sim = np.zeros((n_x, n_x), np.double)
    x_biases_ = x_biases
    y_biases_ = y_biases
            min_sprt = max(2, min_sprt)
    for y, y_ratings in iteritems(yr):
        partial_bias = global_mean_ + y_biases_[y]
        for xi, ri in y_ratings:
            for xj, rj in y_ratings:
                freq[xi, xj] += 1
                diff_i = (ri - (partial_bias + x_biases_[xi]))
                diff_j = (rj - (partial_bias + x_biases_[xj]))
                prods[xi, xj] += diff_i * diff_j
                sq_diff_i[xi, xj] += diff_i**2
                sq_diff_j[xi, xj] += diff_j**2
    for xi in range(n_x):
        sim[xi, xi] = 1
        for xj in range(xi + 1, n_x):
            if freq[xi, xj] < min_sprt:
                sim[xi, xj] = 0
            else:
                sim[xi, xj] = prods[xi, xj] / (np.sqrt(sq_diff_i[xi, xj] *
                                                       sq_diff_j[xi, xj]))
                                sim[xi, xj] *= (freq[xi, xj] - 1) / (freq[xi, xj] - 1 +
                                                     shrinkage)
            sim[xj, xi] = sim[xi, xj]
    return sim
from pkg_resources import get_distribution
from .prediction_algorithms import AlgoBase
from .prediction_algorithms import NormalPredictor
from .prediction_algorithms import BaselineOnly
from .prediction_algorithms import KNNBasic
from .prediction_algorithms import KNNWithMeans
from .prediction_algorithms import KNNBaseline
from .prediction_algorithms import SVD
from .prediction_algorithms import SVDpp
from .prediction_algorithms import NMF
from .prediction_algorithms import SlopeOne
from .prediction_algorithms import CoClustering
from .prediction_algorithms import PredictionImpossible
from .prediction_algorithms import Prediction
from .dataset import Dataset
from .dataset import Reader
from .dataset import Trainset
from .evaluate import evaluate
from .evaluate import print_perf
from .evaluate import GridSearch
from .dump import dump
__all__ = ['AlgoBase', 'NormalPredictor', 'BaselineOnly', 'KNNBasic',
           'KNNWithMeans', 'KNNBaseline', 'SVD', 'SVDpp', 'NMF', 'SlopeOne',
           'CoClustering', 'PredictionImpossible', 'Prediction', 'Dataset',
           'Reader', 'Trainset', 'evaluate', 'print_perf', 'GridSearch',
           'dump']
__version__ = get_distribution('scikit-surprise').version
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import random as rd
import sys
import shutil
import argparse
import numpy as np
from surprise.prediction_algorithms import NormalPredictor
from surprise.prediction_algorithms import BaselineOnly
from surprise.prediction_algorithms import KNNBasic
from surprise.prediction_algorithms import KNNBaseline
from surprise.prediction_algorithms import KNNWithMeans
from surprise.prediction_algorithms import SVD
from surprise.prediction_algorithms import SVDpp
from surprise.prediction_algorithms import NMF
from surprise.prediction_algorithms import SlopeOne
from surprise.prediction_algorithms import CoClustering
import surprise.dataset as dataset
from surprise.dataset import Dataset
from surprise.dataset import Reader  from surprise.evaluate import evaluate
from surprise import __version__

def main():
    class MyParser(argparse.ArgumentParser):
        '''A parser which prints the help message when an error occurs. Taken from
        http://stackoverflow.com/questions/4042452/display-help-message-with-python-argparse-when-script-is-called-without-any-argu.'''  
        def error(self, message):
            sys.stderr.write('error: %s\n' % message)
            self.print_help()
            sys.exit(2)
    parser = MyParser(
        description='Evaluate the performance of a rating prediction ' +
        'on a given dataset using cross validation. You can use a built-in ' +
        'or a custom dataset, and you can choose to automatically split the ' +
        'dataset into folds, or manually specify train and test files. ' +
        'Please refer to the documentation page ' +
        '(http://surprise.readthedocs.io/) for more details.',
        epilog=The :mod:`surprise.prediction_algorithms.algo_base` module defines the base
class :class:`AlgoBase` from which every single prediction algorithm has to
inherit.
    def __init__(self, **kwargs):
        self.bsl_options = kwargs.get('bsl_options', {})
        self.sim_options = kwargs.get('sim_options', {})
        if 'user_based' not in self.sim_options:
            self.sim_options['user_based'] = True
    def train(self, trainset):
        
        self.trainset = trainset
                self.bu = self.bi = None
    def predict(self, uid, iid, r_ui, clip=True, verbose=False):
        
                try:
            iuid = self.trainset.to_inner_uid(uid)
        except ValueError:
            iuid = 'UKN__' + str(uid)
        try:
            iiid = self.trainset.to_inner_iid(iid)
        except ValueError:
            iiid = 'UKN__' + str(iid)
        details = {}
        try:
            est = self.estimate(iuid, iiid)
                        if isinstance(est, tuple):
                est, details = est
            details['was_impossible'] = False
        except PredictionImpossible as e:
            est = self.trainset.global_mean
            details['was_impossible'] = True
            details['reason'] = str(e)
                        est -= self.trainset.offset
                if clip:
            lower_bound, higher_bound = self.trainset.rating_scale
            est = min(higher_bound, est)
            est = max(lower_bound, est)
        pred = Prediction(uid, iid, r_ui, est, details)
        if verbose:
            print(pred)
        return pred
    def test(self, testset, verbose=False):
        
                predictions = [self.predict(uid,
                                    iid,
                                    r_ui_trans - self.trainset.offset,
                                    verbose=verbose)
                       for (uid, iid, r_ui_trans) in testset]
        return predictions
    def compute_baselines(self):
        
                                        if self.bu is not None:
            return self.bu, self.bi
        method = dict(als=baseline_als,
                      sgd=baseline_sgd)
        method_name = self.bsl_options.get('method', 'als')
        try:
            print('Estimating biases using', method_name + '...')
            self.bu, self.bi = method[method_name](self)
            return self.bu, self.bi
        except KeyError:
            raise ValueError('Invalid method ' + method_name +
                             ' for baseline computation.' +
                             ' Available methods are als and sgd.')
    def compute_similarities(self):
        
        construction_func = {'cosine': sims.cosine,
                             'msd': sims.msd,
                             'pearson': sims.pearson,
                             'pearson_baseline': sims.pearson_baseline}
        if self.sim_options['user_based']:
            n_x, yr = self.trainset.n_users, self.trainset.ir
        else:
            n_x, yr = self.trainset.n_items, self.trainset.ur
        min_support = self.sim_options.get('min_support', 1)
        args = [n_x, yr, min_support]
        name = self.sim_options.get('name', 'msd').lower()
        if name == 'pearson_baseline':
            shrinkage = self.sim_options.get('shrinkage', 100)
            bu, bi = self.compute_baselines()
            if self.sim_options['user_based']:
                bx, by = bu, bi
            else:
                bx, by = bi, bu
            args += [self.trainset.global_mean, bx, by, shrinkage]
        try:
            print('Computing the {0} similarity matrix...'.format(name))
            sim = construction_func[name](*args)
            print('Done computing similarity matrix.')
            return sim
        except KeyError:
            raise NameError('Wrong sim name ' + name + '. Allowed values ' +
                            'are ' + ', '.join(construction_func.keys()) + '.')
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from .algo_base import AlgoBase

class BaselineOnly(AlgoBase):
    
    def __init__(self, bsl_options={}):
        AlgoBase.__init__(self, bsl_options=bsl_options)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.bu, self.bi = self.compute_baselines()
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class CoClustering(AlgoBase):
    
    def __init__(self, n_cltr_u=3, n_cltr_i=3, n_epochs=20, verbose=False):
        AlgoBase.__init__(self)
        self.n_cltr_u = n_cltr_u
        self.n_cltr_i = n_cltr_i
        self.n_epochs = n_epochs
        self.verbose=verbose
    def train(self, trainset):
                
        AlgoBase.train(self, trainset)
                cdef np.ndarray[np.double_t] user_mean
        cdef np.ndarray[np.double_t] item_mean
                cdef np.ndarray[np.int_t] cltr_u
        cdef np.ndarray[np.int_t] cltr_i
                cdef np.ndarray[np.double_t] avg_cltr_u
        cdef np.ndarray[np.double_t] avg_cltr_i
        cdef np.ndarray[np.double_t, ndim=2] avg_cocltr
        cdef np.ndarray[np.double_t] errors
        cdef int u, i, r, uc, ic
        cdef double est
                cltr_u = np.random.randint(self.n_cltr_u, size=trainset.n_users)
        cltr_i = np.random.randint(self.n_cltr_i, size=trainset.n_items)
                user_mean = np.zeros(self.trainset.n_users, np.double)
        item_mean = np.zeros(self.trainset.n_items, np.double)
        for u in trainset.all_users():
            user_mean[u] = np.mean([r for (_, r) in trainset.ur[u]])
        for i in trainset.all_items():
            item_mean[i] = np.mean([r for (_, r) in trainset.ir[i]])
                        for epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(epoch))
                        avg_cltr_u, avg_cltr_i, avg_cocltr = self.compute_averages(cltr_u,
                                                                       cltr_i)
                                    for u in self.trainset.all_users():
                errors = np.zeros(self.n_cltr_u, np.double)
                for uc in range(self.n_cltr_u):
                    for i, r in self.trainset.ur[u]:
                        ic = cltr_i[i]
                        est = (avg_cocltr[uc, ic] +
                               user_mean[u] - avg_cltr_u[uc] +
                               item_mean[i] - avg_cltr_i[ic])
                        errors[uc] += (r - est)**2
                cltr_u[u] = np.argmin(errors)
                                    for i in self.trainset.all_items():
                errors = np.zeros(self.n_cltr_i, np.double)
                for ic in range(self.n_cltr_i):
                    for u, r in self.trainset.ir[i]:
                        uc = cltr_u[u]
                        est = (avg_cocltr[uc, ic] +
                               user_mean[u] - avg_cltr_u[uc] +
                               item_mean[i] - avg_cltr_i[ic])
                        errors[ic] += (r - est)**2
                cltr_i[i] = np.argmin(errors)
                avg_cltr_u, avg_cltr_i, avg_cocltr = self.compute_averages(cltr_u,
                                                                   cltr_i)
                self.cltr_u = cltr_u
        self.cltr_i = cltr_i
        self.user_mean = user_mean
        self.item_mean = item_mean
        self.avg_cltr_u = avg_cltr_u
        self.avg_cltr_i = avg_cltr_i
        self.avg_cocltr = avg_cocltr
    def compute_averages(self, np.ndarray[np.int_t] cltr_u,
                         np.ndarray[np.int_t] cltr_i):
        
                cdef np.ndarray[np.int_t] count_cltr_u
        cdef np.ndarray[np.int_t] count_cltr_i
        cdef np.ndarray[np.int_t, ndim=2] count_cocltr
                cdef np.ndarray[np.int_t] sum_cltr_u
        cdef np.ndarray[np.int_t] sum_cltr_i
        cdef np.ndarray[np.int_t, ndim=2] sum_cocltr
                cdef np.ndarray[np.double_t] avg_cltr_u
        cdef np.ndarray[np.double_t] avg_cltr_i
        cdef np.ndarray[np.double_t, ndim=2] avg_cocltr
        cdef int u, i, r, uc, ic
        cdef double global_mean = self.trainset.global_mean
                count_cltr_u = np.zeros(self.n_cltr_u, np.int)
        count_cltr_i = np.zeros(self.n_cltr_i, np.int)
        count_cocltr = np.zeros((self.n_cltr_u, self.n_cltr_i), np.int)
        sum_cltr_u = np.zeros(self.n_cltr_u, np.int)
        sum_cltr_i = np.zeros(self.n_cltr_i, np.int)
        sum_cocltr = np.zeros((self.n_cltr_u, self.n_cltr_i), np.int)
        avg_cltr_u = np.zeros(self.n_cltr_u, np.double)
        avg_cltr_i = np.zeros(self.n_cltr_i, np.double)
        avg_cocltr = np.zeros((self.n_cltr_u, self.n_cltr_i), np.double)
                for u, i, r in self.trainset.all_ratings():
            uc = cltr_u[u]
            ic = cltr_i[i]
            count_cltr_u[uc] += 1
            count_cltr_i[ic] += 1
            count_cocltr[uc, ic] += 1
            sum_cltr_u[uc] += r
            sum_cltr_i[ic] += r
            sum_cocltr[uc, ic] += r
                for uc in range(self.n_cltr_u):
            if count_cltr_u[uc]:
                avg_cltr_u[uc] = sum_cltr_u[uc] / count_cltr_u[uc]
            else:
                avg_cltr_u[uc] = global_mean
                for ic in range(self.n_cltr_i):
            if count_cltr_i[ic]:
                avg_cltr_i[ic] = sum_cltr_i[ic] / count_cltr_i[ic]
            else:
                avg_cltr_i[ic] = global_mean
                for uc in range(self.n_cltr_u):
            for ic in range(self.n_cltr_i):
                if count_cocltr[uc, ic]:
                    avg_cocltr[uc, ic] = (sum_cocltr[uc, ic] /
                                          count_cocltr[uc, ic])
                else:
                    avg_cocltr[uc, ic] = global_mean
        return avg_cltr_u, avg_cltr_i, avg_cocltr
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            return self.trainset.global_mean
        if not self.trainset.knows_user(u):
            return self.cltr_i[i]
        if not self.trainset.knows_item(i):
            return self.cltr_u[u]
                        cdef int _u = u
        cdef int _i = i
        cdef int uc = self.cltr_u[_u]
        cdef int ic = self.cltr_i[_i]
        cdef double est
        est = (self.avg_cocltr[uc, ic] +
               self.user_mean[_u] - self.avg_cltr_u[uc] +
               self.item_mean[_i] - self.avg_cltr_i[ic])
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from six import iteritems
from .predictions import PredictionImpossible
from .algo_base import AlgoBase

class SymmetricAlgo(AlgoBase):
    
    def __init__(self, sim_options={}, **kwargs):
        AlgoBase.__init__(self, sim_options=sim_options, **kwargs)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        ub = self.sim_options['user_based']
        self.n_x = self.trainset.n_users if ub else self.trainset.n_items
        self.n_y = self.trainset.n_items if ub else self.trainset.n_users
        self.xr = self.trainset.ur if ub else self.trainset.ir
        self.yr = self.trainset.ir if ub else self.trainset.ur
    def switch(self, u_stuff, i_stuff):
        
        if self.sim_options['user_based']:
            return u_stuff, i_stuff
        else:
            return i_stuff, u_stuff

class KNNBasic(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, **kwargs):
        SymmetricAlgo.__init__(self, sim_options=sim_options, **kwargs)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
        x, y = self.switch(u, i)
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
                sum_sim = sum_ratings = actual_k = 0
        for (_, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                sum_ratings += sim * r
                actual_k += 1
        if actual_k < self.min_k:
            raise PredictionImpossible('Not enough neighbors.')
        est = sum_ratings / sum_sim
        details = {'actual_k': actual_k}
        return est, details

class KNNWithMeans(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, **kwargs):
        SymmetricAlgo.__init__(self, sim_options=sim_options, **kwargs)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.sim = self.compute_similarities()
        self.means = np.zeros(self.n_x)
        for x, ratings in iteritems(self.xr):
            self.means[x] = np.mean([r for (_, r) in ratings])
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
        x, y = self.switch(u, i)
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
        est = self.means[x]
                sum_sim = sum_ratings = actual_k = 0
        for (nb, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                sum_ratings += sim * (r - self.means[nb])
                actual_k += 1
        if actual_k < self.min_k:
            sum_ratings = 0
        try:
            est += sum_ratings / sum_sim
        except ZeroDivisionError:
            pass  
        details = {'actual_k': actual_k}
        return est, details

class KNNBaseline(SymmetricAlgo):
    
    def __init__(self, k=40, min_k=1, sim_options={}, bsl_options={}):
        SymmetricAlgo.__init__(self, sim_options=sim_options,
                               bsl_options=bsl_options)
        self.k = k
        self.min_k = min_k
    def train(self, trainset):
        SymmetricAlgo.train(self, trainset)
        self.bu, self.bi = self.compute_baselines()
        self.bx, self.by = self.switch(self.bu, self.bi)
        self.sim = self.compute_similarities()
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        x, y = self.switch(u, i)
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            return est
        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]
                neighbors = sorted(neighbors, key=lambda tple: tple[1], reverse=True)
                sum_sim = sum_ratings = actual_k = 0
        for (nb, sim, r) in neighbors[:self.k]:
            if sim > 0:
                sum_sim += sim
                nb_bsl = self.trainset.global_mean + self.bx[nb] + self.by[y]
                sum_ratings += sim * (r - nb_bsl)
                actual_k += 1
        if actual_k < self.min_k:
            sum_ratings = 0
        try:
            est += sum_ratings / sum_sim
        except ZeroDivisionError:
            pass  
        details = {'actual_k': actual_k}
        return est, details
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class SVD(AlgoBase):
    
    def __init__(self, n_factors=100, n_epochs=20, biased=True, lr_all=.005,
                 reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.biased = biased
        self.lr_bu = lr_bu if lr_bu is not None else lr_all
        self.lr_bi = lr_bi if lr_bi is not None else lr_all
        self.lr_pu = lr_pu if lr_pu is not None else lr_all
        self.lr_qi = lr_qi if lr_qi is not None else lr_all
        self.reg_bu = reg_bu if reg_bu is not None else reg_all
        self.reg_bi = reg_bi if reg_bi is not None else reg_all
        self.reg_pu = reg_pu if reg_pu is not None else reg_all
        self.reg_qi = reg_qi if reg_qi is not None else reg_all
        self.verbose = verbose
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                                                                                                                                                                                                        
                                        
                cdef np.ndarray[np.double_t] bu
                cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] pu
                cdef np.ndarray[np.double_t, ndim=2] qi
        cdef int u, i, f
        cdef double r, err, dot, puf, qif
        cdef double global_mean = self.trainset.global_mean
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double lr_pu = self.lr_pu
        cdef double lr_qi = self.lr_qi
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        pu = np.zeros((trainset.n_users, self.n_factors), np.double) + .1
        qi = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        if not self.biased:
            global_mean = 0
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(current_epoch))
            for u, i, r in trainset.all_ratings():
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * pu[u, f]
                err = r - (global_mean + bu[u] + bi[i] + dot)
                                if self.biased:
                    bu[u] += lr_bu * (err - reg_bu * bu[u])
                    bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    puf = pu[u, f]
                    qif = qi[i, f]
                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)
                    qi[i, f] += lr_qi * (err * puf - reg_qi * qif)
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
    def estimate(self, u, i):
        
        est = self.trainset.global_mean if self.biased else 0
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            est += np.dot(self.qi[i], self.pu[u])
        else:
            raise PredictionImpossible
        return est

class SVDpp(AlgoBase):
    
    def __init__(self, n_factors=20, n_epochs=20, lr_all=.007, reg_all=.02,
                 lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None, lr_yj=None,
                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,
                 reg_yj=None, verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.lr_bu = lr_bu if lr_bu is not None else lr_all
        self.lr_bi = lr_bi if lr_bi is not None else lr_all
        self.lr_pu = lr_pu if lr_pu is not None else lr_all
        self.lr_qi = lr_qi if lr_qi is not None else lr_all
        self.lr_yj = lr_yj if lr_yj is not None else lr_all
        self.reg_bu = reg_bu if reg_bu is not None else reg_all
        self.reg_bi = reg_bi if reg_bi is not None else reg_all
        self.reg_pu = reg_pu if reg_pu is not None else reg_all
        self.reg_qi = reg_qi if reg_qi is not None else reg_all
        self.reg_yj = reg_yj if reg_yj is not None else reg_all
        self.verbose = verbose
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                cdef np.ndarray[np.double_t] bu
                cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] pu
                cdef np.ndarray[np.double_t, ndim=2] qi
                cdef np.ndarray[np.double_t, ndim=2] yj
        cdef int u, i, j, f
        cdef double r, err, dot, puf, qif, sqrt_Iu, _
        cdef double global_mean = self.trainset.global_mean
        cdef np.ndarray[np.double_t] u_impl_fdb
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double lr_pu = self.lr_pu
        cdef double lr_qi = self.lr_qi
        cdef double lr_yj = self.lr_yj
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        cdef double reg_yj = self.reg_yj
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        pu = np.zeros((trainset.n_users, self.n_factors), np.double) + .1
        qi = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        yj = np.zeros((trainset.n_items, self.n_factors), np.double) + .1
        u_impl_fdb = np.zeros(self.n_factors, np.double)
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print(" processing epoch {}".format(current_epoch))
            for u, i, r in trainset.all_ratings():
                                Iu = [j for (j, _) in trainset.ur[u]]
                sqrt_Iu = np.sqrt(len(Iu))
                                u_impl_fdb = np.zeros(self.n_factors, np.double)
                for j in Iu:
                    for f in range(self.n_factors):
                        u_impl_fdb[f] += yj[j, f] / sqrt_Iu
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * (pu[u, f] + u_impl_fdb[f])
                err = r - (global_mean + bu[u] + bi[i] + dot)
                                bu[u] += lr_bu * (err - reg_bu * bu[u])
                bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    puf = pu[u, f]
                    qif = qi[i, f]
                    pu[u, f] += lr_pu * (err * qif - reg_pu * puf)
                    qi[i, f] += lr_qi * (err * (puf + u_impl_fdb[f]) -
                                         reg_qi * qif)
                    for j in Iu:
                        yj[j, f] += lr_yj * (err * qif / sqrt_Iu -
                                             reg_yj * yj[j, f])
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
        self.yj = yj
    def estimate(self, u, i):
        est = self.trainset.global_mean
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            Iu = len(self.trainset.ur[u])              u_impl_feedback = (sum(self.yj[j] for (j, _)
                               in self.trainset.ur[u]) / np.sqrt(Iu))
            est += np.dot(self.qi[i], self.pu[u] + u_impl_feedback)
        return est

class NMF(AlgoBase):
    
    def __init__(self, n_factors=15, n_epochs=50, biased=False, reg_pu=.06,
                 reg_qi=.06, reg_bu=.02, reg_bi=.02, lr_bu=.005, lr_bi=.005,
                 init_low=0, init_high=1, verbose=False):
        self.n_factors = n_factors
        self.n_epochs = n_epochs
        self.biased = biased
        self.reg_pu = reg_pu
        self.reg_qi = reg_qi
        self.lr_bu = lr_bu
        self.lr_bi = lr_bi
        self.reg_bu = reg_bu
        self.reg_bi = reg_bi
        self.init_low = init_low
        self.init_high = init_high
        self.verbose = verbose
        if self.init_low < 0:
            raise ValueError('init_low should be greater than zero')
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        self.sgd(trainset)
    def sgd(self, trainset):
                cdef np.ndarray[np.double_t, ndim=2] pu
        cdef np.ndarray[np.double_t, ndim=2] qi
                cdef np.ndarray[np.double_t] bu
        cdef np.ndarray[np.double_t] bi
                cdef np.ndarray[np.double_t, ndim=2] user_num
        cdef np.ndarray[np.double_t, ndim=2] user_denom
        cdef np.ndarray[np.double_t, ndim=2] item_num
        cdef np.ndarray[np.double_t, ndim=2] item_denom
        cdef int u, i, f
        cdef double r, est, l, dot, err
        cdef double reg_pu = self.reg_pu
        cdef double reg_qi = self.reg_qi
        cdef double reg_bu = self.reg_bu
        cdef double reg_bi = self.reg_bi
        cdef double lr_bu = self.lr_bu
        cdef double lr_bi = self.lr_bi
        cdef double global_mean = self.trainset.global_mean
                pu = np.random.uniform(self.init_low, self.init_high,
                               size=(trainset.n_users, self.n_factors))
        qi = np.random.uniform(self.init_low, self.init_high,
                               size=(trainset.n_items, self.n_factors))
        bu = np.zeros(trainset.n_users, np.double)
        bi = np.zeros(trainset.n_items, np.double)
        if not self.biased:
            global_mean = 0
        for current_epoch in range(self.n_epochs):
            if self.verbose:
                print("Processing epoch {}".format(current_epoch))
                        user_num = np.zeros((trainset.n_users, self.n_factors))
            user_denom = np.zeros((trainset.n_users, self.n_factors))
            item_num = np.zeros((trainset.n_items, self.n_factors))
            item_denom = np.zeros((trainset.n_items, self.n_factors))
                        for u, i, r in trainset.all_ratings():
                                dot = 0                  for f in range(self.n_factors):
                    dot += qi[i, f] * pu[u, f]
                est = global_mean + bu[u] + bi[i] + dot
                err = r - est
                                if self.biased:
                    bu[u] += lr_bu * (err - reg_bu * bu[u])
                    bi[i] += lr_bi * (err - reg_bi * bi[i])
                                for f in range(self.n_factors):
                    user_num[u, f] += qi[i, f] * r
                    user_denom[u, f] += qi[i, f] * est
                    item_num[i, f] += pu[u, f] * r
                    item_denom[i, f] += pu[u, f] * est
                        for u in trainset.all_users():
                l = len(trainset.ur[u])
                for f in range(self.n_factors):
                    user_denom[u, f] += l * reg_pu * pu[u, f]
                    pu[u, f] *= user_num[u, f] / user_denom[u, f]
                        for i in trainset.all_items():
                l = len(trainset.ir[i])
                for f in range(self.n_factors):
                    item_denom[i, f] += l * reg_qi * qi[i, f]
                    qi[i, f] *= item_num[i, f] / item_denom[i, f]
        self.bu = bu
        self.bi = bi
        self.pu = pu
        self.qi = qi
    def estimate(self, u, i):
        est = self.trainset.global_mean if self.biased else 0
        if self.trainset.knows_user(u):
            est += self.bu[u]
        if self.trainset.knows_item(i):
            est += self.bi[i]
        if self.trainset.knows_user(u) and self.trainset.knows_item(i):
            est += np.dot(self.qi[i], self.pu[u])
        else:
            raise PredictionImpossible
        return est
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range

def baseline_als(self):
    
                
    cdef np.ndarray[np.double_t] bu = np.zeros(self.trainset.n_users)
    cdef np.ndarray[np.double_t] bi = np.zeros(self.trainset.n_items)
    cdef int u, i
    cdef double r, err, dev_i, dev_u
    cdef double global_mean = self.trainset.global_mean
    cdef int n_epochs = self.bsl_options.get('n_epochs', 10)
    cdef double reg_u = self.bsl_options.get('reg_u', 15)
    cdef double reg_i = self.bsl_options.get('reg_i', 10)
    for dummy in range(n_epochs):
        for i in self.trainset.all_items():
            dev_i = 0
            for (u, r) in self.trainset.ir[i]:
                dev_i += r - global_mean - bu[u]
            bi[i] = dev_i / (reg_i + len(self.trainset.ir[i]))
        for u in self.trainset.all_users():
            dev_u = 0
            for (i, r) in self.trainset.ur[u]:
                dev_u += r - global_mean - bi[i]
            bu[u] = dev_u / (reg_u + len(self.trainset.ur[u]))
    return bu, bi

def baseline_sgd(self):
    
    cdef np.ndarray[np.double_t] bu = np.zeros(self.trainset.n_users)
    cdef np.ndarray[np.double_t] bi = np.zeros(self.trainset.n_items)
    cdef int u, i
    cdef double r, err
    cdef double global_mean = self.trainset.global_mean
    cdef int n_epochs = self.bsl_options.get('n_epochs', 20)
    cdef double reg = self.bsl_options.get('reg', 0.02)
    cdef double lr = self.bsl_options.get('learning_rate', 0.005)
    for dummy in range(n_epochs):
        for u, i, r in self.trainset.all_ratings():
            err = (r - (global_mean + bu[u] + bi[i]))
            bu[u] += lr * (err - reg * bu[u])
            bi[i] += lr * (err - reg * bi[i])
    return bu, bi
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
from collections import namedtuple

class PredictionImpossible(Exception):
    
    pass

class Prediction(namedtuple('Prediction',
                            ['uid', 'iid', 'r_ui', 'est', 'details'])):
    
    __slots__ = ()  
    def __str__(self):
        s = 'user: {uid:<10} '.format(uid=self.uid)
        s += 'item: {iid:<10} '.format(iid=self.iid)
        s += 'r_ui = {r_ui:1.2f}   '.format(r_ui=self.r_ui)
        s += 'est = {est:1.2f}   '.format(est=self.est)
        s += str(self.details)
        return s
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
import numpy as np
from .algo_base import AlgoBase

class NormalPredictor(AlgoBase):
    
    def __init__(self):
        AlgoBase.__init__(self)
    def train(self, trainset):
        AlgoBase.train(self, trainset)
        num = sum((r - self.trainset.global_mean)**2
                  for (_, _, r) in self.trainset.all_ratings())
        denum = self.trainset.n_ratings
        self.sigma = np.sqrt(num / denum)
    def estimate(self, *_):
        return np.random.normal(self.trainset.global_mean, self.sigma)
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
cimport numpy as np  import numpy as np
from six.moves import range
from six import iteritems
from .algo_base import AlgoBase
from .predictions import PredictionImpossible

class SlopeOne(AlgoBase):
    
    def __init__(self):
        AlgoBase.__init__(self)
    def train(self, trainset):
        n_items = trainset.n_items
                cdef np.ndarray[np.int_t, ndim=2] freq
                cdef np.ndarray[np.double_t, ndim=2] dev
        cdef int u, i, j, r_ui, r_uj
        AlgoBase.train(self, trainset)
        freq = np.zeros((trainset.n_items, trainset.n_items), np.int)
        dev = np.zeros((trainset.n_items, trainset.n_items), np.double)
                for u, u_ratings in iteritems(trainset.ur):
            for i, r_ui in u_ratings:
                for j, r_uj in u_ratings:
                    freq[i, j] += 1
                    dev[i, j] += r_ui - r_uj
        for i in range(n_items):
            dev[i, i] = 0
            for j in range(i + 1, n_items):
                dev[i, j] /= freq[i, j]
                dev[j, i] = -dev[i, j]
        self.freq = freq
        self.dev = dev
                self.user_mean = [np.mean([r for (_, r) in trainset.ur[u]])
                          for u in trainset.all_users()]
    def estimate(self, u, i):
        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):
            raise PredictionImpossible('User and/or item is unkown.')
                                Ri = [j for (j, _) in self.trainset.ur[u] if self.freq[i, j] > 0]
        est = self.user_mean[u]
        if Ri:
            est += sum(self.dev[i, j] for j in Ri) / len(Ri)
        return est
from .algo_base import AlgoBase
from .random_pred import NormalPredictor
from .baseline_only import BaselineOnly
from .knns import KNNBasic
from .knns import KNNBaseline
from .knns import KNNWithMeans
from .matrix_factorization import SVD
from .matrix_factorization import SVDpp
from .matrix_factorization import NMF
from .slope_one import SlopeOne
from .co_clustering import CoClustering
from .predictions import PredictionImpossible
from .predictions import Prediction
__all__ = ['AlgoBase', 'NormalPredictor', 'BaselineOnly', 'KNNBasic',
           'KNNBaseline', 'KNNWithMeans', 'SVD', 'SVDpp', 'NMF', 'SlopeOne',
           'CoClustering', 'PredictionImpossible', 'Prediction']
from setuptools import setup, find_packages

def calculate_version():
    initpy = open('tpot/_version.py').read().split('\n')
    version = list(filter(lambda x: '__version__' in x, initpy))[0].split('\'')[1]
    return version
package_version = calculate_version()
setup(
    name='TPOT',
    version=package_version,
    author='Randal S. Olson',
    author_email='rso@randalolson.com',
    packages=find_packages(),
    url='https://github.com/rhiever/tpot',
    license='GNU/GPLv3',
    entry_points={'console_scripts': ['tpot=tpot:main', ]},
    description=('Tree-based Pipeline Optimization Tool'),
    long_description='''
A Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.
Contact
=============
If you have any questions or comments about TPOT, please feel free to contact me via:
E-mail: rso@randalolson.com
or Twitter: https://twitter.com/randal_olson
This project is hosted at https://github.com/rhiever/tpot
''',
    zip_safe=True,
    install_requires=['numpy', 'scipy', 'scikit-learn>=0.18.1', 'deap', 'update_checker', 'tqdm', 'pathos'],
    extras_require={'xgboost': ['xgboost']},
    classifiers=[
        'Intended Audience :: Science/Research',
        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Topic :: Scientific/Engineering :: Artificial Intelligence'
    ],
    keywords=['pipeline optimization', 'hyperparameter optimization', 'data science', 'machine learning', 'genetic programming', 'evolutionary computation'],
)

from __future__ import print_function
import random
import inspect
import warnings
import sys
import time
from functools import partial
from datetime import datetime
from pathos.multiprocessing import ProcessPool

import numpy as np
import deap
from deap import base, creator, tools, gp
from tqdm import tqdm
from sklearn.base import BaseEstimator
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.ensemble import VotingClassifier
from sklearn.metrics.scorer import make_scorer
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from update_checker import update_check
from ._version import __version__
from .operator_utils import TPOTOperatorClassFactory, Operator, ARGType, set_sample_weight
from .export_utils import export_pipeline, expr_to_tree, generate_pipeline_code
from .decorators import _timeout, _pre_test, TimedOutExc
from .built_in_operators import CombineDFs
from .metrics import SCORERS
from .gp_types import Output_Array
from .gp_deap import eaMuPlusLambda, mutNodeReplacement
if sys.platform.startswith('win'):
    import win32api
    try:
        import _thread
    except ImportError:
        import thread as _thread
    def handler(dwCtrlType, hook_sigint=_thread.interrupt_main):
        if dwCtrlType == 0:             hook_sigint()
            return 1         return 0
    win32api.SetConsoleCtrlHandler(handler, 1)

class TPOTBase(BaseEstimator):
    
    def __init__(self, generations=100, population_size=100, offspring_size=None,
                 mutation_rate=0.9, crossover_rate=0.1,
                 scoring=None, cv=5, n_jobs=1,
                 max_time_mins=None, max_eval_time_mins=5,
                 random_state=None, config_dict=None, warm_start=False,
                 verbosity=0, disable_update_check=False):
                if self.__class__.__name__ == 'TPOTBase':
            raise RuntimeError('Do not instantiate the TPOTBase class directly; use TPOTRegressor or TPOTClassifier instead.')
                self.disable_update_check = disable_update_check
        if not self.disable_update_check:
            update_check('tpot', __version__)
        self._pareto_front = None
        self._optimized_pipeline = None
        self._fitted_pipeline = None
        self._pop = None
        self.warm_start = warm_start
        self.population_size = population_size
        self.generations = generations
        self.max_time_mins = max_time_mins
        self.max_eval_time_mins = max_eval_time_mins
                if offspring_size:
            self.offspring_size = offspring_size
        else:
            self.offspring_size = population_size
                if config_dict:
            self.config_dict = config_dict
        else:
            self.config_dict = self.default_config_dict
        self.operators = []
        self.arguments = []
        for key in sorted(self.config_dict.keys()):
            op_class, arg_types = TPOTOperatorClassFactory(key, self.config_dict[key],
            BaseClass=Operator, ArgBaseClass=ARGType)
            if op_class:
                self.operators.append(op_class)
                self.arguments += arg_types
                        if not (max_time_mins is None):
            self.generations = 1000000
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        if self.mutation_rate + self.crossover_rate > 1:
            raise ValueError('The sum of the crossover and mutation probabilities must be <= 1.0.')
        self.verbosity = verbosity
        self.operators_context = {
            'make_pipeline': make_pipeline,
            'make_union': make_union,
            'VotingClassifier': VotingClassifier,
            'FunctionTransformer': FunctionTransformer
        }
        self._pbar = None
                self._evaluated_individuals = {}
        self.random_state = random_state
                if scoring:
            if hasattr(scoring, '__call__'):
                scoring_name = scoring.__name__
                if 'loss' in scoring_name or 'error' in scoring_name:
                    greater_is_better = False
                else:
                    greater_is_better = True
                SCORERS[scoring_name] = make_scorer(scoring, greater_is_better=greater_is_better)
                self.scoring_function = scoring_name
            else:
                if scoring not in SCORERS:
                    raise ValueError('The scoring function {} is not available. '
                                     'Please choose a valid scoring function from the TPOT '
                                     'documentation.'.format(scoring))
                self.scoring_function = scoring
        self.cv = cv
                if sys.platform.startswith('win') and n_jobs > 1:
            print('Warning: Parallelization is not currently supported in TPOT for Windows. ',
                  'Setting n_jobs to 1 during the TPOT optimization process.')
            self.n_jobs = 1
        else:
            self.n_jobs = n_jobs
        self._setup_pset()
        self._setup_toolbox()
    def _setup_pset(self):
        if self.random_state is not None:
            random.seed(self.random_state)
            np.random.seed(self.random_state)
        self._pset = gp.PrimitiveSetTyped('MAIN', [np.ndarray], Output_Array)
                self._pset.renameArguments(ARG0='input_matrix')

                for op in self.operators:
            if op.root:
                                                                p_types = (op.parameter_types()[0], Output_Array)
                self._pset.addPrimitive(op, *p_types)
            self._pset.addPrimitive(op, *op.parameter_types())
                                    for key in sorted(op.import_hash.keys()):
                module_list = ', '.join(sorted(op.import_hash[key]))
                if key.startswith('tpot.'):
                    exec('from {} import {}'.format(key[4:], module_list))
                else:
                    exec('from {} import {}'.format(key, module_list))
                for var in op.import_hash[key]:
                    self.operators_context[var] = eval(var)
        self._pset.addPrimitive(CombineDFs(), [np.ndarray, np.ndarray], np.ndarray)
                for _type in self.arguments:
            type_values = list(_type.values) + ['DEFAULT']
            for val in type_values:
                terminal_name = _type.__name__ + "=" + str(val)
                self._pset.addTerminal(val, _type, name=terminal_name)
        if self.verbosity > 2:
            print('{} operators have been imported by TPOT.'.format(len(self.operators)))

    def _setup_toolbox(self):
        creator.create('FitnessMulti', base.Fitness, weights=(-1.0, 1.0))
        creator.create('Individual', gp.PrimitiveTree, fitness=creator.FitnessMulti)
        self._toolbox = base.Toolbox()
        self._toolbox.register('expr', self._gen_grow_safe, pset=self._pset, min_=1, max_=3)
        self._toolbox.register('individual', tools.initIterate, creator.Individual, self._toolbox.expr)
        self._toolbox.register('population', tools.initRepeat, list, self._toolbox.individual)
        self._toolbox.register('compile', self._compile_to_sklearn)
        self._toolbox.register('select', tools.selNSGA2)
        self._toolbox.register('mate', self._mate_operator)
        self._toolbox.register('expr_mut', self._gen_grow_safe, min_=1, max_=4)
        self._toolbox.register('mutate', self._random_mutation_operator)
    def fit(self, features, classes, sample_weight=None):
                features = features.astype(np.float64)
                if self.classification:
            clf = DecisionTreeClassifier(max_depth=5)
        else:
            clf = DecisionTreeRegressor(max_depth=5)
        try:
            clf = clf.fit(features, classes)
        except:
            raise ValueError('Error: Input data is not in a valid format. '
                             'Please confirm that the input data is scikit-learn compatible. '
                             'For example, the features must be a 2-D array and target labels '
                             'must be a 1-D array.')
                if self.random_state is not None:
            random.seed(self.random_state)             np.random.seed(self.random_state)
        self._start_datetime = datetime.now()
        self._toolbox.register('evaluate', self._evaluate_individuals, features=features, classes=classes, sample_weight=sample_weight)
                if self._pop:
            pop = self._pop
        else:
            pop = self._toolbox.population(n=self.population_size)
        def pareto_eq(ind1, ind2):
                        return np.allclose(ind1.fitness.values, ind2.fitness.values)
                if not self.warm_start or not self._pareto_front:
            self._pareto_front = tools.ParetoFront(similar=pareto_eq)
                if self.max_time_mins:
            total_evals = self.population_size
        else:
            total_evals = self.offspring_size * self.generations + self.population_size
        self._pbar = tqdm(total=total_evals, unit='pipeline', leave=False,
                          disable=not (self.verbosity >= 2), desc='Optimization Progress')
        try:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore')
                pop, _ = eaMuPlusLambda(population=pop, toolbox=self._toolbox,
                    mu=self.population_size, lambda_=self.offspring_size,
                    cxpb=self.crossover_rate, mutpb=self.mutation_rate,
                    ngen=self.generations, pbar=self._pbar, halloffame=self._pareto_front,
                    verbose=self.verbosity, max_time_mins=self.max_time_mins)
                        if self.warm_start:
                self._pop = pop
                except (KeyboardInterrupt, SystemExit):
            if self.verbosity > 0:
                self._pbar.write('')                 self._pbar.write('TPOT closed prematurely. Will use the current best pipeline.')
        finally:
                                    if not isinstance(self._pbar, type(None)):
                self._pbar.close()
                        if self._pareto_front:
                top_score = -float('inf')
                for pipeline, pipeline_scores in zip(self._pareto_front.items, reversed(self._pareto_front.keys)):
                    if pipeline_scores.wvalues[1] > top_score:
                        self._optimized_pipeline = pipeline
                        top_score = pipeline_scores.wvalues[1]
                                                if not self._optimized_pipeline:
                    print('There was an error in the TPOT optimization '
                          'process. This could be because the data was '
                          'not formatted properly, or because data for '
                          'a regression problem was provided to the '
                          'TPOTClassifier object. Please make sure you '
                          'passed the data to TPOT correctly.')
                else:
                    self._fitted_pipeline = self._toolbox.compile(expr=self._optimized_pipeline)
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        self._fitted_pipeline.fit(features, classes)
                    if self.verbosity in [1, 2]:
                                                if self.verbosity >= 2:
                            print('')
                        print('Best pipeline: {}'.format(self._optimized_pipeline))
                                        elif self.verbosity >= 3 and self._pareto_front:
                        self._pareto_front_fitted_pipelines = {}
                        for pipeline in self._pareto_front.items:
                            self._pareto_front_fitted_pipelines[str(pipeline)] = self._toolbox.compile(expr=pipeline)
                            with warnings.catch_warnings():
                                warnings.simplefilter('ignore')
                                self._pareto_front_fitted_pipelines[str(pipeline)].fit(features, classes)
    def predict(self, features):
                if not self._fitted_pipeline:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        return self._fitted_pipeline.predict(features.astype(np.float64))
    def fit_predict(self, features, classes):
                self.fit(features, classes)
        return self.predict(features)
    def score(self, testing_features, testing_classes):
                if self._fitted_pipeline is None:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
                return abs(SCORERS[self.scoring_function](self._fitted_pipeline,
            testing_features.astype(np.float64), testing_classes.astype(np.float64)))
    def predict_proba(self, features):
                if not self._fitted_pipeline:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        else:
            if not(hasattr(self._fitted_pipeline, 'predict_proba')):
                raise RuntimeError('The fitted pipeline does not have the predict_proba() function.')
            return self._fitted_pipeline.predict_proba(features.astype(np.float64))
    def set_params(self, **params):
                self.__init__(**params)
        return self
    def export(self, output_file_name):
                if self._optimized_pipeline is None:
            raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
        with open(output_file_name, 'w') as output_file:
            output_file.write(export_pipeline(self._optimized_pipeline, self.operators, self._pset))
    def _compile_to_sklearn(self, expr):
                sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr, self._pset), self.operators)
        return eval(sklearn_pipeline, self.operators_context)
    def _set_param_recursive(self, pipeline_steps, parameter, value):
                for (_, obj) in pipeline_steps:
            recursive_attrs = ['steps', 'transformer_list', 'estimators']
            for attr in recursive_attrs:
                if hasattr(obj, attr):
                    self._set_param_recursive(getattr(obj, attr), parameter, value)
                    break
            else:
                if hasattr(obj, parameter):
                    setattr(obj, parameter, value)
    def _evaluate_individuals(self, individuals, features, classes, sample_weight = None):
                if self.max_time_mins:
            total_mins_elapsed = (datetime.now() - self._start_datetime).total_seconds() / 60.
            if total_mins_elapsed >= self.max_time_mins:
                raise KeyboardInterrupt('{} minutes have elapsed. TPOT will close down.'.format(total_mins_elapsed))
                fitnesses_dict = {}
                eval_individuals_str = []
        sklearn_pipeline_list = []
        operator_count_list = []
        test_idx_list = []
        for indidx, individual in enumerate(individuals):
                                    individual = individuals[indidx]
            individual_str = str(individual)
            if individual_str.count('PolynomialFeatures') > 1:
                if self.verbosity > 2:
                    self._pbar.write('Invalid pipeline encountered. Skipping its evaluation.')
                fitnesses_dict[indidx] = (5000., -float('inf'))
                if not self._pbar.disable:
                    self._pbar.update(1)
                        elif individual_str in self._evaluated_individuals:
                                fitnesses_dict[indidx] = self._evaluated_individuals[individual_str]
                if self.verbosity > 2:
                    self._pbar.write('Pipeline encountered that has previously been evaluated during the '
                                     'optimization process. Using the score from the previous evaluation.')
                if not self._pbar.disable:
                    self._pbar.update(1)
            else:
                try:
                                        sklearn_pipeline = self._toolbox.compile(expr=individual)
                                        self._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
                                        operator_count = 0
                    for i in range(len(individual)):
                        node = individual[i]
                        if ((type(node) is deap.gp.Terminal) or
                             type(node) is deap.gp.Primitive and node.name == 'CombineDFs'):
                            continue
                        operator_count += 1
                except Exception:
                    fitnesses_dict[indidx] = (5000., -float('inf'))
                    if not self._pbar.disable:
                        self._pbar.update(1)
                    continue
                eval_individuals_str.append(individual_str)
                operator_count_list.append(operator_count)
                sklearn_pipeline_list.append(sklearn_pipeline)
                test_idx_list.append(indidx)
        @_timeout(max_eval_time_mins=self.max_eval_time_mins)
        def _wrapped_cross_val_score(sklearn_pipeline, features=features, classes=classes,
                                     cv=self.cv, scoring_function=self.scoring_function,
                                     sample_weight=sample_weight):
            sample_weight_dict = set_sample_weight(sklearn_pipeline.steps, sample_weight)
            from .decorators import TimedOutExc
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    cv_scores = cross_val_score(sklearn_pipeline, features, classes,
                        cv=cv, scoring=scoring_function,
                        n_jobs=1, fit_params=sample_weight_dict)
                resulting_score = np.mean(cv_scores)
            except TimedOutExc:
                resulting_score = 'Timeout'
            except Exception:
                resulting_score = -float('inf')
            return resulting_score
        if not sys.platform.startswith('win'):
            if self.n_jobs == -1:
                pool = ProcessPool()
            else:
                pool = ProcessPool(nodes=self.n_jobs)
            res_imap = pool.imap(_wrapped_cross_val_score, sklearn_pipeline_list)
            if not self._pbar.disable:
                ini_pbar_n = self._pbar.n
                        while not self._pbar.disable:
                tmp_fitness = np.array(res_imap._items)
                num_job_done = len(tmp_fitness)
                if not self._pbar.disable and num_job_done:
                    timeout_index = list(np.where(tmp_fitness[:, 1] == 'Timeout')[0])
                    for idx in timeout_index:
                        if self.verbosity > 2 and self._pbar.n - ini_pbar_n <= idx:
                            self._pbar.write('Skipped pipeline                                              'Continuing to the next pipeline.'.format(ini_pbar_n + idx + 1))
                    self._pbar.update(ini_pbar_n + num_job_done - self._pbar.n)
                if num_job_done >= len(sklearn_pipeline_list):
                    break
                else:
                    time.sleep(0.2)
            resulting_score_list = [-float('inf') if x == 'Timeout' else x for x in list(res_imap)]
        else:
            resulting_score_list = []
            for sklearn_pipeline in sklearn_pipeline_list:
                try:
                    resulting_score = _wrapped_cross_val_score(sklearn_pipeline)
                except TimedOutExc:
                    resulting_score = -float('inf')
                    if self.verbosity > 2 and not self._pbar.disable:
                        self._pbar.write('Skipped pipeline                                          'Continuing to the next pipeline.'.format(self._pbar.n + 1))
                resulting_score_list.append(resulting_score)
                if not self._pbar.disable:
                    self._pbar.update(1)
        for resulting_score, operator_count, individual_str, test_idx in zip(resulting_score_list, operator_count_list, eval_individuals_str, test_idx_list):
            if type(resulting_score) in [float, np.float64, np.float32]:
                self._evaluated_individuals[individual_str] = (max(1, operator_count), resulting_score)
                fitnesses_dict[test_idx] = self._evaluated_individuals[individual_str]
            else:
                raise ValueError('Scoring function does not return a float.')
        fitnesses_ordered = []
        for key in sorted(fitnesses_dict.keys()):
            fitnesses_ordered.append(fitnesses_dict[key])
        return fitnesses_ordered
    @_pre_test
    def _mate_operator(self, ind1, ind2):
        return gp.cxOnePoint(ind1, ind2)
    @_pre_test
    def _random_mutation_operator(self, individual):
                                mutation_techniques = [
            partial(gp.mutInsert, pset=self._pset),
            partial(mutNodeReplacement, pset=self._pset),
            partial(gp.mutShrink)
        ]
        return np.random.choice(mutation_techniques)(individual)
    def _gen_grow_safe(self, pset, min_, max_, type_=None):
                def condition(height, depth, type_):
                        return type_ not in [np.ndarray, Output_Array] or depth == height
        return self._generate(pset, min_, max_, condition, type_)
        @_pre_test
    def _generate(self, pset, min_, max_, condition, type_=None):
                if type_ is None:
            type_ = pset.ret
        expr = []
        height = np.random.randint(min_, max_)
        stack = [(0, type_)]
        while len(stack) != 0:
            depth, type_ = stack.pop()
                        if condition(height, depth, type_):
                try:
                    term = np.random.choice(pset.terminals[type_])
                except IndexError:
                    _, _, traceback = sys.exc_info()
                    raise IndexError("The gp.generate function tried to add "
                                      "a terminal of type '%s', but there is "
                                      "none available." % (type_,)).\
                                      with_traceback(traceback)
                if inspect.isclass(term):
                    term = term()
                expr.append(term)
            else:
                try:
                    prim = np.random.choice(pset.primitives[type_])
                except IndexError:
                    _, _, traceback = sys.exc_info()
                    raise IndexError("The gp.generate function tried to add "
                                      "a primitive of type '%s', but there is "
                                      "none available." % (type_,)).\
                                      with_traceback(traceback)
                expr.append(prim)
                for arg in reversed(prim.args):
                    stack.append((depth+1, arg))
        return expr

import numpy as np
from sklearn.base import BaseEstimator
from sklearn.utils import check_array

class ZeroCount(BaseEstimator):
    
    def __init__(self):
        pass
    def fit(self, X, y=None):
                return self
    def transform(self, X, y=None):
                X = check_array(X)
        n_features = X.shape[1]
        X_transformed = np.copy(X)
        non_zero = np.apply_along_axis(lambda row: np.count_nonzero(row),
                                        axis=1, arr=X_transformed)
        zero_col = np.apply_along_axis(lambda row: (n_features - np.count_nonzero(row)),
                                        axis=1, arr=X_transformed)
        X_transformed = np.insert(X_transformed, n_features, non_zero, axis=1)
        X_transformed = np.insert(X_transformed, n_features + 1, zero_col, axis=1)
        return X_transformed
class CombineDFs(object):
    
    @property
    def __name__(self):
        return self.__class__.__name__
import numpy as np
classifier_config_dict = {
        'sklearn.naive_bayes.GaussianNB': {
    },
    'sklearn.naive_bayes.BernoulliNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },
    'sklearn.naive_bayes.MultinomialNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },
    'sklearn.tree.DecisionTreeClassifier': {
        'criterion': ["gini", "entropy"],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21)
    },
    'sklearn.ensemble.ExtraTreesClassifier': {
        'criterion': ["gini", "entropy"],
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.RandomForestClassifier': {
        'criterion': ["gini", "entropy"],
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf':  range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.GradientBoostingClassifier': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'subsample': np.arange(0.05, 1.01, 0.05),
        'max_features': np.arange(0, 1.01, 0.05)
    },
    'sklearn.neighbors.KNeighborsClassifier': {
        'n_neighbors': range(1, 101),
        'weights': ["uniform", "distance"],
        'p': [1, 2]
    },
    'sklearn.svm.LinearSVC': {
        'penalty': ["l1", "l2"],
        'loss': ["hinge", "squared_hinge"],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]
    },
    'sklearn.linear_model.LogisticRegression': {
        'penalty': ["l1", "l2"],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'dual': [True, False]
    },
    'xgboost.XGBClassifier': {
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21)
    },
        'sklearn.preprocessing.Binarizer': {
        'threshold': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.decomposition.FastICA': {
        'tol': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.cluster.FeatureAgglomeration': {
        'linkage': ['ward', 'complete', 'average'],
        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']
    },
    'sklearn.preprocessing.MaxAbsScaler': {
    },
    'sklearn.preprocessing.MinMaxScaler': {
    },
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },
    'sklearn.kernel_approximation.Nystroem': {
        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],
        'gamma': np.arange(0.0, 1.01, 0.05),
        'n_components': range(1, 11)
    },
    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },
    'sklearn.kernel_approximation.RBFSampler': {
        'gamma': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.preprocessing.RobustScaler': {
    },
    'sklearn.preprocessing.StandardScaler': {
    },
    'tpot.built_in_operators.ZeroCount': {
    },
        'sklearn.feature_selection.SelectFwe': {
        'alpha': np.arange(0, 0.05, 0.001),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            } 
    },
    'sklearn.feature_selection.SelectKBest': {
        'k': range(1, 100),         'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': np.arange(0.05, 1.01, 0.05)
    },
    'sklearn.feature_selection.RFE': {
        'step': np.arange(0.05, 1.01, 0.05),
        'estimator': {
            'sklearn.svm.SVC': {
                'kernel': ['linear'],
                'random_state': [42]
                }
        }
    },
   'sklearn.feature_selection.SelectFromModel': {
        'threshold': np.arange(0, 1.01, 0.05),
        'estimator': {
            'sklearn.ensemble.ExtraTreesClassifier': {
                'criterion': ['gini', 'entropy'],
                'max_features': np.arange(0, 1.01, 0.05)
                }
        }
    }
}
import numpy as np
regressor_config_dict = {

    'sklearn.linear_model.ElasticNetCV': {
        'l1_ratio': np.arange(0.0, 1.01, 0.05),
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
    },
    'sklearn.ensemble.ExtraTreesRegressor': {
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.ensemble.GradientBoostingRegressor': {
        'loss': ["ls", "lad", "huber", "quantile"],
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'subsample': np.arange(0.05, 1.01, 0.05),
        'max_features': np.arange(0, 1.01, 0.05),
        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]
    },
    'sklearn.ensemble.AdaBoostRegressor': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'loss': ["linear", "square", "exponential"],
        'max_depth': range(1, 11)
    },
    'sklearn.tree.DecisionTreeRegressor': {
        'max_depth': range(1, 11),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21)
    },
    'sklearn.neighbors.KNeighborsRegressor': {
        'n_neighbors': range(1, 101),
        'weights': ["uniform", "distance"],
        'p': [1, 2]
    },
    'sklearn.linear_model.LassoLarsCV': {
        'normalize': [True, False]
    },
    'sklearn.svm.LinearSVR': {
        'loss': ["epsilon_insensitive", "squared_epsilon_insensitive"],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1.]
    },
    'sklearn.ensemble.RandomForestRegressor': {
        'max_features': np.arange(0, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    'sklearn.linear_model.RidgeCV': {
    },

    'xgboost.XGBRegressor': {
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21)
    },
        'sklearn.preprocessing.Binarizer': {
        'threshold': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.decomposition.FastICA': {
        'tol': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.cluster.FeatureAgglomeration': {
        'linkage': ['ward', 'complete', 'average'],
        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed']
    },
    'sklearn.preprocessing.MaxAbsScaler': {
    },
    'sklearn.preprocessing.MinMaxScaler': {
    },
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },
    'sklearn.kernel_approximation.Nystroem': {
        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],
        'gamma': np.arange(0.0, 1.01, 0.05),
        'n_components': range(1, 11)
    },
    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },
    'sklearn.kernel_approximation.RBFSampler': {
        'gamma': np.arange(0.0, 1.01, 0.05)
    },
    'sklearn.preprocessing.RobustScaler': {
    },
    'sklearn.preprocessing.StandardScaler': {
    },
    'tpot.built_in_operators.ZeroCount': {
    },
        'sklearn.feature_selection.SelectFwe': {
        'alpha': np.arange(0, 0.05, 0.001),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            } 
    },
    'sklearn.feature_selection.SelectKBest': {
        'k': range(1, 100),         'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
            }
    },
    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': np.arange(0.05, 1.01, 0.05)
    },
    'sklearn.feature_selection.SelectFromModel': {
        'threshold': np.arange(0, 1.01, 0.05),
        'estimator': {
                'sklearn.ensemble.ExtraTreesRegressor': {
                    'max_features': np.arange(0, 1.01, 0.05)
                    }
                }
    }
}

from __future__ import print_function
from threading import Thread, current_thread
from functools import wraps
import sys
import warnings
from sklearn.datasets import make_classification, make_regression
from .export_utils import expr_to_tree, generate_pipeline_code
from deap import creator
pretest_X, pretest_y = make_classification(n_samples=50, n_features=10, random_state=42)
pretest_X_reg, pretest_y_reg = make_regression(n_samples=50, n_features=10, random_state=42)

def convert_mins_to_secs(time_minute):
            return max(int(time_minute * 60), 1)

class TimedOutExc(RuntimeError):
    
def timeout_signal_handler(signum, frame):
        raise TimedOutExc("Time Out!")
def _timeout(max_eval_time_mins=5):
        def wrap_func(func):
        if not sys.platform.startswith('win'):
            import signal
            @wraps(func)
            def limitedTime(*args, **kw):
                old_signal_hander = signal.signal(signal.SIGALRM, timeout_signal_handler)
                max_time_seconds = convert_mins_to_secs(max_eval_time_mins)
                signal.alarm(max_time_seconds)
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        ret = func(*args, **kw)
                except:
                    raise TimedOutExc('Time Out!')
                finally:
                    signal.signal(signal.SIGALRM, old_signal_hander)                      signal.alarm(0)                  return ret
        else:
            class InterruptableThread(Thread):
                def __init__(self, args, kwargs):
                    Thread.__init__(self)
                    self.args = args
                    self.kwargs = kwargs
                    self.result = -float('inf')
                    self.daemon = True
                def stop(self):
                    self._stop()
                def run(self):
                                                            current_thread().name = 'MainThread'
                    with warnings.catch_warnings():
                        warnings.simplefilter('ignore')
                        self.result = func(*self.args, **self.kwargs)
            @wraps(func)
            def limitedTime(*args, **kwargs):
                sys.tracebacklimit = 0
                max_time_seconds = convert_mins_to_secs(max_eval_time_mins)
                                tmp_it = InterruptableThread(args, kwargs)
                tmp_it.start()
                                tmp_it.join(max_time_seconds)
                if tmp_it.isAlive():
                    raise TimedOutExc('Time Out!')
                sys.tracebacklimit = 1000
                tmp_it.stop()
                return tmp_it.result
        return limitedTime
    return wrap_func


def _pre_test(func):
        @wraps(func)
    def check_pipeline(self, *args, **kwargs):
        bad_pipeline = True
        num_test = 0         while bad_pipeline and num_test < 10:                         args = [self._toolbox.clone(arg) if isinstance(arg, creator.Individual) else arg for arg in args]
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter('ignore')
                    expr = func(self, *args, **kwargs)
                                        expr_tuple = expr if isinstance(expr, tuple) else (expr,)
                    for expr_test in expr_tuple:
                                                sklearn_pipeline = eval(generate_pipeline_code(expr_to_tree(expr_test, self._pset), self.operators), self.operators_context)
                        if self.classification:
                            sklearn_pipeline.fit(pretest_X, pretest_y)
                        else:
                            sklearn_pipeline.fit(pretest_X_reg, pretest_y_reg)
                        bad_pipeline = False
            except BaseException as e:
                if self.verbosity > 2:
                    print_function = print
                                        if not isinstance(self._pbar, type(None)):
                        print_function = self._pbar.write
                    print_function('_pre_test decorator: {fname}: num_test={n} {e}'.format(n=num_test, fname=func.__name__, e=e))
            finally:
                num_test += 1
        return expr
    return check_pipeline

import numpy as np
import argparse
from sklearn.model_selection import train_test_split
from .tpot import TPOTClassifier, TPOTRegressor
from ._version import __version__

def positive_integer(value):
        try:
        value = int(value)
    except Exception:
        raise argparse.ArgumentTypeError('Invalid int value: \'{}\''.format(value))
    if value < 0:
        raise argparse.ArgumentTypeError('Invalid positive int value: \'{}\''.format(value))
    return value

def float_range(value):
        try:
        value = float(value)
    except:
        raise argparse.ArgumentTypeError('Invalid float value: \'{}\''.format(value))
    if value < 0.0 or value > 1.0:
        raise argparse.ArgumentTypeError('Invalid float value: \'{}\''.format(value))
    return value

def main():
        parser = argparse.ArgumentParser(description='A Python tool that '
        'automatically creates and optimizes machine learning pipelines using '
        'genetic programming.', add_help=False)
    parser.add_argument('INPUT_FILE', type=str, help='Data file to use in the TPOT '
        'optimization process. Ensure that the class label column is labeled as "class".')
    parser.add_argument('-h', '--help', action='help',
        help='Show this help message and exit.')
    parser.add_argument('-is', action='store', dest='INPUT_SEPARATOR', default='\t',
        type=str, help='Character used to separate columns in the input file.')
    parser.add_argument('-target', action='store', dest='TARGET_NAME', default='class',
        type=str, help='Name of the target column in the input file.')
    parser.add_argument('-mode', action='store', dest='TPOT_MODE',
        choices=['classification', 'regression'], default='classification', type=str,
        help='Whether TPOT is being used for a supervised classification or regression problem.')
    parser.add_argument('-o', action='store', dest='OUTPUT_FILE', default='',
        type=str, help='File to export the code for the final optimized pipeline.')
    parser.add_argument('-g', action='store', dest='GENERATIONS', default=100,
        type=positive_integer, help='Number of iterations to run the pipeline optimization process.\n'
        'Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline. '
        'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.')
    parser.add_argument('-p', action='store', dest='POPULATION_SIZE', default=100,
        type=positive_integer, help='Number of individuals to retain in the GP population every generation.\n'
        'Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline. '
        'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.')
    parser.add_argument('-os', action='store', dest='OFFSPRING_SIZE', default=None,
        type=positive_integer, help='Number of offspring to produce in each GP generation. '
        'By default, OFFSPRING_SIZE = POPULATION_SIZE.')
    parser.add_argument('-mr', action='store', dest='MUTATION_RATE', default=0.9,
        type=float_range, help='GP mutation rate in the range [0.0, 1.0]. This tells the '
        'GP algorithm how many pipelines to apply random changes to every generation. '
        'We recommend using the default parameter unless you understand how the mutation '
        'rate affects GP algorithms.')
    parser.add_argument('-xr', action='store', dest='CROSSOVER_RATE', default=0.1,
        type=float_range, help='GP crossover rate in the range [0.0, 1.0]. This tells the '
        'GP algorithm how many pipelines to "breed" every generation. '
        'We recommend using the default parameter unless you understand how the crossover '
        'rate affects GP algorithms.')
    parser.add_argument('-scoring', action='store', dest='SCORING_FN', default=None,
        type=str, help='Function used to evaluate the quality of a given pipeline for '
        'the problem. By default, accuracy is used for classification problems and mean '
        'squared error (mse) is used for regression problems. '
        'TPOT assumes that any function with "error" or "loss" in the name is meant to '
        'be minimized, whereas any other functions will be maximized. '
        'Offers the same options as cross_val_score: '
        '"accuracy", "adjusted_rand_score", "average_precision", "f1", "f1_macro", '
        '"f1_micro", "f1_samples", "f1_weighted", "log_loss", "mean_absolute_error", '
        '"mean_squared_error", "median_absolute_error", "precision", "precision_macro", '
        '"precision_micro", "precision_samples", "precision_weighted", "r2", "recall", '
        '"recall_macro", "recall_micro", "recall_samples", "recall_weighted", "roc_auc"')
    parser.add_argument('-cv', action='store', dest='NUM_CV_FOLDS', default=5,
        type=int, help='Number of folds to evaluate each pipeline over in '
        'k-fold cross-validation during the TPOT optimization process.')
    parser.add_argument('-njobs', action='store', dest='NUM_JOBS', default=1,
        type=int, help='Number of CPUs for evaluating pipelines in parallel '
        ' during the TPOT optimization process. Assigning this to -1 will use as many '
        'cores as available on the computer.')
    parser.add_argument('-maxtime', action='store', dest='MAX_TIME_MINS', default=None,
        type=int, help='How many minutes TPOT has to optimize the pipeline. This '
        'setting will override the GENERATIONS parameter '
        'and allow TPOT to run until it runs out of time.')
    parser.add_argument('-maxeval', action='store', dest='MAX_EVAL_MINS', default=5,
        type=float, help='How many minutes TPOT has to evaluate a single pipeline. '
        'Setting this parameter to higher values will allow TPOT to explore more complex '
        'pipelines but will also allow TPOT to run longer.')
    parser.add_argument('-s', action='store', dest='RANDOM_STATE', default=None,
        type=int, help='Random number generator seed for reproducibility. Set '
        'this seed if you want your TPOT run to be reproducible with the same '
        'seed and data set in the future.')
    parser.add_argument('-config', action='store', dest='CONFIG_FILE', default='',
        type=str, help='Configuration file for customizing the operators and parameters '
        'that TPOT uses in the optimization process.')
    parser.add_argument('-v', action='store', dest='VERBOSITY', default=1,
        choices=[0, 1, 2, 3], type=int, help='How much information TPOT communicates '
        'while it is running: 0 = none, 1 = minimal, 2 = high, 3 = all. '
        'A setting of 2 or higher will add a progress bar during the optimization procedure.')
    parser.add_argument('--no-update-check', action='store_true',
        dest='DISABLE_UPDATE_CHECK', default=False,
        help='Flag indicating whether the TPOT version checker should be disabled.')
    parser.add_argument('--version', action='version',
        version='TPOT {version}'.format(version=__version__),
        help='Show the TPOT version number and exit.')
    args = parser.parse_args()
    if args.VERBOSITY >= 2:
        print('\nTPOT settings:')
        for arg in sorted(args.__dict__):
            arg_val = args.__dict__[arg]
            if arg == 'DISABLE_UPDATE_CHECK':
                continue
            elif arg == 'SCORING_FN' and arg_val is None:
                if args.TPOT_MODE == 'classification':
                    arg_val = 'accuracy'
                else:
                    arg_val = 'mean_squared_error'
            elif arg == 'OFFSPRING_SIZE' and arg_val is None:
                arg_val = args.__dict__['POPULATION_SIZE']
            print('{}\t=\t{}'.format(arg, arg_val))
        print('')
    input_data = np.recfromcsv(args.INPUT_FILE, delimiter=args.INPUT_SEPARATOR, dtype=np.float64, case_sensitive=True)
    if args.TARGET_NAME not in input_data.dtype.names:
        raise ValueError('The provided data file does not seem to have a target column. '
                         'Please make sure to specify the target column using the -target parameter.')
    features = np.delete(input_data.view(np.float64).reshape(input_data.size, -1),
                         input_data.dtype.names.index(args.TARGET_NAME), axis=1)
    training_features, testing_features, training_classes, testing_classes = \
        train_test_split(features, input_data[args.TARGET_NAME], random_state=args.RANDOM_STATE)
    if args.TPOT_MODE == 'classification':
        tpot_type = TPOTClassifier
    else:
        tpot_type = TPOTRegressor
    operator_dict = None
    if args.CONFIG_FILE:
        try:
            with open(args.CONFIG_FILE, 'r') as input_file:
                file_string =  input_file.read()
            operator_dict = eval(file_string[file_string.find('{'):(file_string.rfind('}') + 1)])
        except:
            raise TypeError('The operator configuration file is in a bad format or not available. '
                            'Please check the configuration file before running TPOT.')
    tpot = tpot_type(generations=args.GENERATIONS, population_size=args.POPULATION_SIZE,
                     offspring_size=args.OFFSPRING_SIZE, mutation_rate=args.MUTATION_RATE, crossover_rate=args.CROSSOVER_RATE,
                     cv=args.NUM_CV_FOLDS, n_jobs=args.NUM_JOBS,
                     scoring=args.SCORING_FN,
                     max_time_mins=args.MAX_TIME_MINS, max_eval_time_mins=args.MAX_EVAL_MINS,
                     random_state=args.RANDOM_STATE, config_dict=operator_dict,
                     verbosity=args.VERBOSITY, disable_update_check=args.DISABLE_UPDATE_CHECK)
    print('')
    tpot.fit(training_features, training_classes)
    if args.VERBOSITY in [1, 2] and tpot._optimized_pipeline:
        training_score = max([tpot._pareto_front.keys[x].wvalues[1] for x in range(len(tpot._pareto_front.keys))])
        print('\nTraining score: {}'.format(abs(training_score)))
        print('Holdout score: {}'.format(tpot.score(testing_features, testing_classes)))
    elif args.VERBOSITY >= 3 and tpot._pareto_front:
        print('Final Pareto front testing scores:')
        for pipeline, pipeline_scores in zip(tpot._pareto_front.items, reversed(tpot._pareto_front.keys)):
            tpot._fitted_pipeline = tpot._pareto_front_fitted_pipelines[str(pipeline)]
            print('{}\t{}\t{}'.format(int(abs(pipeline_scores.wvalues[0])),
                                      tpot.score(testing_features, testing_classes),
                                      pipeline))
    if args.OUTPUT_FILE != '':
        tpot.export(args.OUTPUT_FILE)

if __name__ == '__main__':
    main()

import deap
def get_by_name(opname, operators):
        ret_op_classes = [op for op in operators if op.__name__ == opname]
    if len(ret_op_classes) == 0:
        raise TypeError('Cannot found operator {} in operator dictionary'.format(opname))
    elif len(ret_op_classes) > 1:
        print('Found multiple operator {} in operator dictionary'.format(opname),
        'Please check your dictionary file.')
    ret_op_class = ret_op_classes[0]
    return ret_op_class
def export_pipeline(exported_pipeline, operators, pset):
            pipeline_tree = expr_to_tree(exported_pipeline, pset)
        pipeline_text = generate_import_code(exported_pipeline, operators)
        pipeline_text += pipeline_code_wrapper(generate_export_pipeline_code(pipeline_tree, operators))
    return pipeline_text

def expr_to_tree(ind, pset):
        def prim_to_list(prim, args):
        if isinstance(prim, deap.gp.Terminal):
            if prim.name in pset.context:
                 return pset.context[prim.name]
            else:
                 return prim.value
        return [prim.name] + args
    tree = []
    stack = []
    for node in ind:
        stack.append((node, []))
        while len(stack[-1][1]) == stack[-1][0].arity:
            prim, args = stack.pop()
            tree = prim_to_list(prim, args)
            if len(stack) == 0:
                break               stack[-1][1].append(tree)
    return tree

def generate_import_code(pipeline, operators):
            operators_used = [x.name for x in pipeline if isinstance(x, deap.gp.Primitive)]
    pipeline_text = 'import numpy as np\n\n'
        num_op = len(operators_used)
        import_relations = {}
    for op in operators:
        import_relations[op.__name__] = op.import_hash
        num_op_root = 0
    for op in operators_used:
        if op != 'CombineDFs':
            tpot_op = get_by_name(op, operators)
            if tpot_op.root:
                num_op_root += 1
        else:
            num_op_root += 1
        if num_op_root > 1:
        pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline', 'make_union'],
            'sklearn.preprocessing':    ['FunctionTransformer'],
            'sklearn.ensemble':         ['VotingClassifier']
        }
    elif num_op > 1:
        pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline']
        }
    else:         pipeline_imports = {
            'sklearn.model_selection':  ['train_test_split']
        }
        for op in operators_used:
        def merge_imports(old_dict, new_dict):
                        for key in new_dict.keys():
                if key in old_dict.keys():
                                        old_dict[key] = set(old_dict[key]) | set(new_dict[key])
                else:
                    old_dict[key] = set(new_dict[key])
        try:
            operator_import = import_relations[op]
            merge_imports(pipeline_imports, operator_import)
        except KeyError:
            pass  
        for key in sorted(pipeline_imports.keys()):
        module_list = ', '.join(sorted(pipeline_imports[key]))
        pipeline_text += 'from {} import {}\n'.format(key, module_list)
    pipeline_text += 
    return pipeline_text

def pipeline_code_wrapper(pipeline_code):
        return     steps = process_operator(pipeline_tree, operators)
    pipeline_text = "make_pipeline(\n{STEPS}\n)".format(STEPS=_indent(",\n".join(steps), 4))
    return pipeline_text
def generate_export_pipeline_code(pipeline_tree, operators):
        steps = process_operator(pipeline_tree, operators)
        num_step = len(steps)
    if num_step > 1:
        pipeline_text = "make_pipeline(\n{STEPS}\n)".format(STEPS=_indent(",\n".join(steps), 4))
    else:         pipeline_text =  "{STEPS}".format(STEPS=_indent(",\n".join(steps), 0))
    return pipeline_text
def process_operator(operator, operators, depth=0):
    steps = []
    op_name = operator[0]
    if op_name == "CombineDFs":
        steps.append(
            _combine_dfs(operator[1], operator[2], operators)
        )
    else:
        input_name, args = operator[1], operator[2:]
        tpot_op = get_by_name(op_name, operators)
        if input_name != 'input_matrix':
            steps.extend(process_operator(input_name, operators, depth + 1))
                        if tpot_op.root and depth > 0:
            steps.append(
                "make_union(VotingClassifier([(\"est\", {})]), FunctionTransformer(lambda X: X))".
                format(tpot_op.export(*args))
            )
        else:
            steps.append(tpot_op.export(*args))
    return steps

def _indent(text, amount):
        indentation = amount * ' '
    return indentation + ('\n' + indentation).join(text.split('\n'))

def _combine_dfs(left, right, operators):
    def _make_branch(branch):
        if branch == "input_matrix":
            return "FunctionTransformer(lambda X: X)"
        elif branch[0] == "CombineDFs":
            return _combine_dfs(branch[1], branch[2], operators)
        elif branch[1] == "input_matrix":              tpot_op = get_by_name(branch[0], operators)
            if tpot_op.root:
                return Copyright 2015-Present Randal S. Olson
This file is modified based on codes for alogrithms.eaSimple module in DEAP.
This file is part of the TPOT library.
The TPOT library is free software: you can redistribute it and/or
modify it under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your option)
any later version.
The TPOT library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
details. You should have received a copy of the GNU General Public License along
with the TPOT library. If not, see http://www.gnu.org/licenses/.
    offspring = []
    for _ in range(lambda_):
        op_choice = np.random.random()
        if op_choice < cxpb:                        idxs = np.random.randint(0, len(population),size=2)
            ind1, ind2 = toolbox.clone(population[idxs[0]]), toolbox.clone(population[idxs[1]])
            ind_str = str(ind1)
            num_loop = 0
            while ind_str == str(ind1) and num_loop < 50 :                 ind1, ind2 = toolbox.mate(ind1, ind2)
                num_loop += 1
            if ind_str != str(ind1):                 del ind1.fitness.values
            offspring.append(ind1)
        elif op_choice < cxpb + mutpb:              idx = np.random.randint(0, len(population))
            ind = toolbox.clone(population[idx])
            ind_str = str(ind)
            num_loop = 0
            while ind_str == str(ind) and num_loop < 50 :                 ind, = toolbox.mutate(ind)
                num_loop += 1
            if ind_str != str(ind):                 del ind.fitness.values
            offspring.append(ind)
        else:             idx = np.random.randint(0, len(population))
            offspring.append(toolbox.clone(population[idx]))
    return offspring
def eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar,
                   stats=None, halloffame=None, verbose=0, max_time_mins = None):
        logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])
        invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = toolbox.evaluate(invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit
    if halloffame is not None:
        halloffame.update(population)
    record = stats.compile(population) if stats is not None else {}
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
        for gen in range(1, ngen + 1):
                offspring = varOr(population, toolbox, lambda_, cxpb, mutpb)
                invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
                if not pbar.disable:
            pbar.update(len(offspring)-len(invalid_ind))
            if not (max_time_mins is None) and pbar.n >= pbar.total:
                pbar.total += lambda_
        fitnesses = toolbox.evaluate(invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            ind.fitness.values = fit

                if halloffame is not None:
            halloffame.update(offspring)
                population[:] = toolbox.select(population + offspring, mu)
                if not pbar.disable:
                        if verbose == 2:
                high_score = abs(max([halloffame.keys[x].wvalues[1] for x in range(len(halloffame.keys))]))
                pbar.write('Generation {0} - Current best internal CV score: {1}'.format(gen, high_score))
                        elif verbose == 3:
                pbar.write('Generation {} - Current Pareto front scores:'.format(gen))
                for pipeline, pipeline_scores in zip(halloffame.items, reversed(halloffame.keys)):
                    pbar.write('{}\t{}\t{}'.format(int(abs(pipeline_scores.wvalues[0])),
                                                         abs(pipeline_scores.wvalues[1]),
                                                         pipeline))
                pbar.write('')
                record = stats.compile(population) if stats is not None else {}
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)
    return population, logbook

def mutNodeReplacement(individual, pset):
    
    index = np.random.randint(0, len(individual))
    node = individual[index]
    slice_ = individual.searchSubtree(index)
    if node.arity == 0:          term = np.random.choice(pset.terminals[node.ret])
        if isclass(term):
            term = term()
        individual[index] = term
    else:                   rindex = None
        if index + 1 < len(individual):
            for i, tmpnode in enumerate(individual[index+1:], index+ 1):
                if isinstance(tmpnode, gp.Primitive) and tmpnode.ret in tmpnode.args:
                    rindex = i
                                primitives = pset.primitives[node.ret]
        if len(primitives) != 0:
            new_node = np.random.choice(primitives)
            new_subtree = [None] * len(new_node.args)
            if rindex:
                rnode = individual[rindex]
                rslice = individual.searchSubtree(rindex)
                                position = np.random.choice([i for i, a in enumerate(new_node.args) if a == rnode.ret])
            else:
                position = None
            for i, arg_type in enumerate(new_node.args):
                if i != position:
                    term = np.random.choice(pset.terminals[arg_type])
                    if isclass(term):
                        term = term()
                    new_subtree[i] = term
                        if rindex:
                new_subtree[position:position + 1] = individual[rslice]
                        new_subtree.insert(0, new_node)
            individual[slice_] = new_subtree
    return individual,

class Output_Array(object):
    
    pass

import numpy as np
from sklearn.metrics import make_scorer, SCORERS

def balanced_accuracy(y_true, y_pred):
        all_classes = list(set(np.append(y_true, y_pred)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = \
            float(sum((y_pred == this_class) & (y_true == this_class))) /\
            float(sum((y_true == this_class)))
        this_class_specificity = \
            float(sum((y_pred != this_class) & (y_true != this_class))) /\
            float(sum((y_true != this_class)))
        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        all_class_accuracies.append(this_class_accuracy)
    return np.mean(all_class_accuracies)
SCORERS['balanced_accuracy'] = make_scorer(balanced_accuracy)

import numpy as np
from sklearn.base import ClassifierMixin
from sklearn.base import RegressorMixin
import inspect

class Operator(object):
        def __init__(self):
        pass
    root = False      import_hash = None
    sklearn_class = None
    arg_types = None
    dep_op_list = {} 
class ARGType(object):
        def __init__(self):
     pass

def source_decode(sourcecode):
        tmp_path = sourcecode.split('.')
    op_str = tmp_path.pop()
    import_str = '.'.join(tmp_path)
    try:
        if sourcecode.startswith('tpot.'):
            exec('from {} import {}'.format(import_str[4:], op_str))
        else:
            exec('from {} import {}'.format(import_str, op_str))
        op_obj = eval(op_str)
    except ImportError:
        print('Warning: {} is not available and will not be used by TPOT.'.format(sourcecode))
        op_obj = None
    return import_str, op_str, op_obj
def set_sample_weight(pipeline_steps, sample_weight=None):
        sample_weight_dict = {}
    if not isinstance(sample_weight, type(None)):
        for (pname, obj) in pipeline_steps:
            if inspect.getargspec(obj.fit).args.count('sample_weight'):
                step_sw = pname + '__sample_weight'
                sample_weight_dict[step_sw] = sample_weight
    if sample_weight_dict:
        return sample_weight_dict
    else:
        return None
def ARGTypeClassFactory(classname, prange, BaseClass=ARGType):
        return type(classname, (BaseClass,), {'values':prange})
def TPOTOperatorClassFactory(opsourse, opdict, BaseClass=Operator, ArgBaseClass=ARGType):
    
    class_profile = {}
    dep_op_list = {}
    import_str, op_str, op_obj = source_decode(opsourse)
    if not op_obj:
        return None, None     else:
                if issubclass(op_obj, ClassifierMixin) or issubclass(op_obj, RegressorMixin):
            class_profile['root'] = True
            optype = "Classifier or Regressor"
        else:
            optype = "Preprocessor or Selector"
        @classmethod
        def op_type(cls):
                        return optype
        class_profile['type'] = op_type
        class_profile['sklearn_class'] = op_obj
        import_hash = {}
        import_hash[import_str] = [op_str]
        arg_types = []
        for pname in sorted(opdict.keys()):
            prange = opdict[pname]
            if not isinstance(prange, dict):
                classname = '{}__{}'.format(op_str, pname)
                arg_types.append(ARGTypeClassFactory(classname, prange))
            else:
                for dkey, dval in prange.items():
                    dep_import_str, dep_op_str, dep_op_obj = source_decode(dkey)
                    if dep_import_str in import_hash:
                        import_hash[import_str].append(dep_op_str)
                    else:
                        import_hash[dep_import_str] = [dep_op_str]
                    dep_op_list[pname]=dep_op_str
                    if dval:
                        for dpname in sorted(dval.keys()):
                            dprange = dval[dpname]
                            classname = '{}__{}__{}'.format(op_str, dep_op_str, dpname)
                            arg_types.append(ARGTypeClassFactory(classname, dprange))
        class_profile['arg_types'] = tuple(arg_types)
        class_profile['import_hash'] = import_hash
        class_profile['dep_op_list'] = dep_op_list
        @classmethod
        def parameter_types(cls):
                        return ([np.ndarray] + arg_types, np.ndarray)

        class_profile['parameter_types'] = parameter_types
        @classmethod
        def export(cls, *args):
            
            op_arguments = []
            if dep_op_list:
                dep_op_arguments = {}
            for arg_class, arg_value in zip(arg_types, args):
                if arg_value == "DEFAULT":
                    continue
                aname_split = arg_class.__name__.split('__')
                if isinstance(arg_value, str):
                    arg_value = '\"{}\"'.format(arg_value)
                if len(aname_split) == 2:                     op_arguments.append("{}={}".format(aname_split[-1], arg_value))
                else:                     if not list(dep_op_list.values()).count(aname_split[1]):
                        raise TypeError('Warning: the operator {} is not in right format in the operator dictionary'.format(aname_split[0]))
                    else:
                        if aname_split[1] not in dep_op_arguments:
                            dep_op_arguments[aname_split[1]] = []
                        dep_op_arguments[aname_split[1]].append("{}={}".format(aname_split[-1], arg_value))
            tmp_op_args = []
            if dep_op_list:
                                for dep_op_pname, dep_op_str in dep_op_list.items():
                    if dep_op_str == 'f_classif':
                        arg_value = dep_op_str
                    else:
                        arg_value = "{}({})".format(dep_op_str, ", ".join(dep_op_arguments[dep_op_str]))
                    tmp_op_args.append("{}={}".format(dep_op_pname, arg_value))
            op_arguments = tmp_op_args + op_arguments
            return "{}({})".format(op_obj.__name__, ", ".join(op_arguments))
        class_profile['export'] = export
        op_classname = 'TPOT_{}'.format(op_str)
        op_class = type(op_classname, (BaseClass,), class_profile)
        op_class.__name__ = op_str
        return op_class, arg_types

from .base import TPOTBase
from .config_classifier import classifier_config_dict
from .config_regressor import regressor_config_dict

class TPOTClassifier(TPOTBase):
    
    scoring_function = 'accuracy'      default_config_dict = classifier_config_dict     classification = True
    regression = False

class TPOTRegressor(TPOTBase):
    
    scoring_function = 'neg_mean_squared_error'      default_config_dict = regressor_config_dict     classification = False
    regression = True

__version__ = '0.7.0'

from ._version import __version__
from .tpot import TPOTClassifier, TPOTRegressor
from .driver import main
import numpy as np
from sklearn.kernel_approximation import RBFSampler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = make_pipeline(
    RBFSampler(gamma=0.8500000000000001),
    DecisionTreeClassifier(criterion="entropy", max_depth=3, min_samples_leaf=4, min_samples_split=9)
)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = KNeighborsClassifier(n_neighbors=4, p=2, weights="distance")
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)
exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)
exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)
result1 = tpot_data.copy()
gbc1 = GradientBoostingClassifier(learning_rate=0.49, max_features=1.0, min_weight_fraction_leaf=0.09, n_estimators=500, random_state=42)
gbc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['gbc1-classification'] = gbc1.predict(result1.drop('class', axis=1).values)
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import PassiveAggressiveClassifier
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)
result1 = tpot_data.copy()
pagr1 = PassiveAggressiveClassifier(C=0.81, loss="squared_hinge", fit_intercept=True, random_state=42)
pagr1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['pagr1-classification'] = pagr1.predict(result1.drop('class', axis=1).values)
